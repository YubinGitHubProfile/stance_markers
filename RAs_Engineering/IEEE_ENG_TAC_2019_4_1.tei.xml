<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /opt/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Assessment of Depression Based on Visual Cues: A Systematic Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Pampouchidou</surname></persName>
							<email>anastasia.pampouchidou@gmail.com</email>
							<idno type="ORCID">0000-0002-6844-2521</idno>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><forename type="middle">G</forename><surname>Simos</surname></persName>
							<idno type="ORCID">0000-0002-0116-8564</idno>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Marias</surname></persName>
							<email>kmarias@ics.forth.gr</email>
							<idno type="ORCID">0000-0001-8454-1450</idno>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Meriaudeau</surname></persName>
							<email>fabrice.meriaudeau@utp.edu.my.m.pediaditis</email>
							<idno type="ORCID">0000-0002-8656-9913</idno>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<email>fanyang@u-bourgogne.fr.</email>
							<idno type="ORCID">0000-0002-6844-2521</idno>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Pediaditis</surname></persName>
							<email>matthew.pediaditis@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Tsiknakis</surname></persName>
							<email>tsiknaki@ie.teicrete.gr.</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Le2i Laboratory</orgName>
								<orgName type="institution">University of Burgundy</orgName>
								<address>
									<postCode>21078</postCode>
									<settlement>Le Creusot, Dijon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Division of Psychiatry</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Univer-sity of Crete</orgName>
								<address>
									<postCode>GR-70013</postCode>
									<settlement>Heraklion</settlement>
									<region>Crete</region>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Technological Educational Insti-tute of Crete</orgName>
								<orgName type="department" key="dep2">Department of Informatics Engineering</orgName>
								<address>
									<postCode>714 10</postCode>
									<settlement>Heraklion</settlement>
									<region>Crete</region>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Institute of Computer Science</orgName>
								<orgName type="department" key="dep2">Foundation for Research &amp; Technology-Hellas</orgName>
								<address>
									<postCode>GR-70013</postCode>
									<settlement>Heraklion</settlement>
									<region>Crete</region>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Le2i Laboratory</orgName>
								<orgName type="institution">University of Burgundy</orgName>
								<address>
									<postCode>21078</postCode>
									<settlement>Le Creusot, Dijon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">CISIR, Electrical Engineering DepartmentUniversiti Teknologi Petronas</orgName>
								<address>
									<postCode>32600</postCode>
									<settlement>Bota</settlement>
									<region>Perak</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department" key="dep1">Institute of Computer Science</orgName>
								<orgName type="department" key="dep2">Foundation for Research &amp; Technology-Hellas</orgName>
								<address>
									<postCode>GR-70013</postCode>
									<settlement>Heraklion</settlement>
									<region>Crete</region>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Assessment of Depression Based on Visual Cues: A Systematic Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TAFFC.2017.2724035</idno>
					<note type="submission">received 7 Oct. 2015; revised 2 May 2017; accepted 28 June 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-03-16T04:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Depression assessment</term>
					<term>affective computing</term>
					<term>facial expression</term>
					<term>machine learning</term>
					<term>facial image analysis</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Automatic depression assessment based on visual cues is a rapidly growing research domain. The present exhaustive review of existing approaches as reported in over sixty publications during the last ten years focuses on image processing and machine learning algorithms. Visual manifestations of depression, various procedures used for data collection, and existing datasets are summarized. The review outlines methods and algorithms for visual feature extraction, dimensionality reduction, decision methods for classification and regression approaches, as well as different fusion strategies. A quantitative meta-analysis of reported results, relying on performance metrics robust to chance, is included, identifying general trends and key unresolved issues to be considered in future studies of automatic depression assessment utilizing visual cues alone or in combination with vocal or verbal cues.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE present work is a systematic review of existing methods for automatic detection and/or severity assessment of depression. Emphasis is given to approaches utilizing visual signs from the image processing and machine learning perspective in an attempt to fill the gap of previous comprehensive reviews. The aim of the review is to examine methods for automated depression analysis, which could assist clinicians in the diagnosis and monitoring of depression. The main questions addressed are whether: (a) video-based depression assessment can assist the diagnosis and monitoring of the disorder, and (b) if visual cues alone are sufficient or if they need to be supplemented by information from other modalities. State of the art methods are presented highlighting their advantages and limitations, based on a quantitative meta-analysis of their results. Datasets created to serve the various studies and the corresponding data acquisition protocols are also described and discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Clinical Background of Depression</head><p>Depression is the most common mood disorder characterized by persistent negative affect <ref type="bibr" target="#b1">[1]</ref>. Clinically distinct depressive disorders encompass a wide range of manifestations. According to the Diagnostic and Statistical Manual of Mental Disorders of the American Psychiatric Association (APA) <ref type="bibr" target="#b2">[2]</ref>, now in its fifth edition (DSM-5), subtypes of depressive disorders include: Major Depressive Disorder (MDD), Persistent Depressive Disorder (Dysthymia), Disruptive Mood Dysregulation Disorder (DMDD), Premenstrual Dysphoric Disorder (PDD), Substance/Medication-Induced Depressive Disorder (S/M-IDD), Depressive Disorder Due to Another Medical Condition (DDDAMC), and Other Specified Depressive Disorder (OSDD) or Unspecified Depressive Disorder (UDD).</p><p>According to DSM-5 MDD, commonly referred to as Clinical Depression, can be diagnosed by the presence of a) depressed mood most of the day, and/or b) markedly diminished interest or pleasure, combined with at least four of the following symptoms for a period exceeding two weeks:</p><p>Significant weight change of over 5 percent in a month Sleeping disturbances (insomnia or hypersomnia) Psychomotor agitation or retardation almost every day Fatigue or loss of energy almost every day Feelings of worthlessness or excessive guilt Diminished ability to concentrate or indecisiveness almost every day Recurrent thoughts of death or suicidal ideation An additional common feature of all depressive disorders is "(...) clinically significant distress or impairment in social, occupational, or other important areas of functioning (...)" <ref type="bibr" target="#b2">[2]</ref>. With MDD considered as the most typical form of the disease, other depressive disorders share some of the MDD symptoms, while each is distinguished by additional characteristics. For instance, chronicity of symptoms characterizes Dysthymia, also known as chronic depression. DMDD, also chronic, is characterized by severe persistent irritability and recurrent outbursts. PDD requires occurrence of depressive symptoms over a minimum of two menstrual cycles. The onset of depressive symptomatology should be clearly linked to persistent use or withdrawal from substances in order to justify diagnosis of S/M-IDD. Similarly, there should be a clear link between another serious medical condition and emergence of depressive symptomatology in DDDAMC. A diagnosis of Other Specified or Unspecified Depressive Disorders is reserved for cases where full criteria are not met for any of the aforementioned depressive disorders.</p><p>MDD is reported to be the fourth most prominent cause of disability and is expected to become the second by 2020 due to its increasing prevalence <ref type="bibr" target="#b1">[1]</ref>, <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>. The "Survey of Health, Ageing and Retirement in Europe" <ref type="bibr" target="#b5">[5]</ref> documents a consistent rise of depression among adults with increasing age, which is associated with significantly elevated risk for suicidal behavior <ref type="bibr" target="#b6">[6]</ref>. The ongoing economic crisis in Europe resulting in high unemployment is implicated as a trigger, since 70-76 percent of unemployed people have been reported to display significant depressive symptomatology <ref type="bibr" target="#b7">[7]</ref>. Further studies have shown that the economic burden of MDD has increased during the 2005-2010 period by 21.5 percent in the US, while in Europe the cost is estimated at 1 percent of Gross Domestic Product <ref type="bibr" target="#b8">[8]</ref>. The total cost of MDD in 2010 in 30 European countries was estimated at 91.9 billion euros <ref type="bibr" target="#b9">[9]</ref>.</p><p>Etiology of MDD is attributed to a combination of biological factors, environmental/family stressors, and personal vulnerabilities (i.e., psychoemotional/behavioral traits). Epidemiological studies have identified gender, age, and marital status as key demographic factors affecting the onset of MDD <ref type="bibr" target="#b10">[10]</ref>. Genetic factors, early childhood adversity, and premorbid personality characteristics have also been suggested as predisposing MDD factors <ref type="bibr" target="#b11">[11]</ref>. Perfectionism, low self-esteem, and maladaptive coping strategies <ref type="bibr" target="#b12">[12]</ref> are among the key related personality traits.</p><p>A structured clinical interview based on DSM criteria is the standard procedure for depression diagnosis <ref type="bibr" target="#b13">[13]</ref>. Quantification of the presence and severity of depressive symptomatology is often aided by rating scales completed by a specially trained mental health professional in the context of the clinical interview. The Hamilton Depression Rating Scale (HDRS or HAM-D), also known as Hamilton Rating Scale for Depression (HRSD), is one of the most popular scales in clinical settings. HAM-D assesses the severity of 17 related symptoms, such as depressed mood, suicidal ideation, insomnia, work and interests, psychomotor retardation, agitation, anxiety, and somatic symptoms <ref type="bibr" target="#b14">[14]</ref>. Both HAM-D and DSM clinical criteria have been criticized regarding their reliability <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, as diagnosis of MDD is not as consistent as other common medical conditions <ref type="bibr" target="#b17">[17]</ref>. In general, "there is no blood test" for depression <ref type="bibr" target="#b18">[18]</ref> as the disorder lacks biological gold standards <ref type="bibr" target="#b19">[19]</ref>.</p><p>Even recent classification schemes (e.g., DSM-5) run the risk of confusing normal sadness (e.g., bereavement) with depression, raising the likelihood of false positive diagnoses <ref type="bibr" target="#b4">[4]</ref>. Depression assessment is a complex process and diagnosis is associated with a significant degree of uncertainty, given the lack of objective boundaries, and the need to evaluate symptoms within the person's current psychosocial context and past history <ref type="bibr" target="#b20">[20]</ref>. Diagnostic accuracy typically improves when results from successive clinical assessments, performed over several months, are taken into account <ref type="bibr" target="#b21">[21]</ref>. Importantly, a simple "symptom checklist" approach is severely limited and diagnosis requires considerable time investment in order to develop rapport with the patient <ref type="bibr" target="#b18">[18]</ref>. The validity and clinical significance of strict classification schemes has also been questioned <ref type="bibr" target="#b22">[22]</ref>. For instance, MDD has been questioned as a "homogeneous categorical entity" <ref type="bibr" target="#b11">[11]</ref>, and the notion of a "continuum of depressive disorders" is often advocated <ref type="bibr" target="#b23">[23]</ref>. These reasonable concerns go beyond the scope of the present review, given that currently affective computing research relies heavily upon established clinical practice tools and procedures.</p><p>Clinical diagnosis of depression may also be supported by scores on self-report scales and inventories (Self-RIs). Most often used Self-RIs in affective computing research are the varius forms of PHQ-2/8/9 (Patient Health Questionnaire, comprised of 2, 8, or 9 items, respectively) and Beck's Depression Inventory (BDI); Depression and Somatic Symptoms Scale (DSSS) was also used in one study. Self-RIs are convenient and economical, with reported sensitivity and specificity approaching 80-90 percent (e.g., PHQ-9 <ref type="bibr" target="#b24">[24]</ref>), but encompass certain disadvantages. Importantly, they do not take into account the clinical significance of reported symptoms, and do not permit adjustments for individual trait characteristics, other psychiatric and medical comorbidities, and potentially important life events, as opposed to a clinical interview <ref type="bibr" target="#b25">[25]</ref>. Moreover, Self-RIs are limited in their capacity to differentiate between depression subtypes <ref type="bibr" target="#b26">[26]</ref>. Additionally Self-RIs are vulnerable to intentional (such as norm defiance) or unintentional reporting bias (e.g., subjective, central tendency [i.e., avoiding extreme responses], social desirability, and acquiescence) <ref type="bibr" target="#b27">[27]</ref>. In sum, although Self-RIs alone are insufficient to support the diagnosis of depression <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b31">[30]</ref>, they are widely used for screening purposes in various settings, including primary health care. While the costeffectiveness of widespread screening practices for improving the quality of depression care is debated <ref type="bibr" target="#b32">[31]</ref>, practical issues related to the aforementioned limitations of Self-RIs raise questions regarding the overall utility and effectiveness of this practice for population-based mental health.</p><p>Objective measures of psychoemotional state, which are implicitly desirable in clinical and research applications alike <ref type="bibr" target="#b33">[32]</ref>, <ref type="bibr" target="#b34">[33]</ref>, could complement Self-RIs and help overcome some of their shortcomings. Certain Self-RIs are sufficiently brief and can be completed on a regular basis (e.g., monthly or weekly) as part of electronic platforms designed to support long-term monitoring of persons at risk. As suggested by Girard and Cohn <ref type="bibr" target="#b35">[34]</ref>, technological advances in the field have paved the way for viable automated methods for measuring signs of depression with multiple potential clinical applications. Thus, decision support systems capable of capturing and interpreting nonverbal depression-related cues, combined with verbal reports (Self-RIs), could be valuable in both clinical and research applications. In principle, such measures may reduce or even eliminate report bias. In addition, such measures are minimally invasive and do not require extra effort on the part of the respondent, thus likely to increase long-term compliance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Investigating Automatic Depression Assessment</head><p>Current technological means can provide the infrastructure for continuous monitoring of the psychoemotional state in high-risk individuals as part of early detection and/or relapse prevention programs, such as the SEMEOTICONS 1 EU funded project, aiming to provide reliable indices of anxiety/stress-related cardiometabolic risk <ref type="bibr" target="#b46">[49]</ref>, <ref type="bibr" target="#b47">[50]</ref>. A system devoted to the assessment of depressive symptomatology based on visual cues, should likewise provide reliable indices, partly based on facial expression analysis, in an unobtrusive manner. Currently, video-based systems for depression assessment have only been found in research-related projects, and have not been applied in the general population to evaluate their feasibility. Although currently limited to research applications, the field has been very popular, with a dedicated section within the "Audio/Visual Emotion Challenge" (AVEC). AVEC'13 had three papers accepted for the "Depression Recognition Sub-challenge" (DSC) <ref type="bibr" target="#b48">[51]</ref>, <ref type="bibr" target="#b49">[52]</ref>, while AVEC'14 <ref type="bibr" target="#b50">[53]</ref> respectively attracted 44 submissions by 13 teams worldwide; the latest AVEC (2016), attracted submissions from 7 teams for the DSC <ref type="bibr" target="#b51">[54]</ref>. Apart from being an active field drawing broad interest, AVEC submissions document the sheer number of research groups working towards the development of such methods. This fact implies that the idea of developing automated depression assessment methods is not only promising, but is continuously progressing towards more robust and reliable measures. Furthermore, the widespread and relatively low-cost accessibility to computer and internet technologies, webcams, and smart phones, renders an efficient system for depression assessment viable. Practical issues involved in developing such a system, like the storage of sensitive data, could become an issue if not handled properly, but there are ways to tackle such challenges; encryption, protection by password, or even an authorization procedure could be implemented to regulate access to sensitive personal data.</p><p>In parallel, a great number of Web-based tools for depression management have been developed and used clinically displaying a high degree of acceptance and patient adherence <ref type="bibr" target="#b52">[55]</ref>. However, as it will become apparent in subsequent sections, given the current state of-the-art, video-based systems for depression assessment are not intended as standalone tools, but mainly to function as a decision support systems assisting mental health professionals in the monitoring of persons at risk. "Behaviormedics" is the term Valstar <ref type="bibr" target="#b53">[56]</ref> introduced for applications designed for the automatic analysis of affective and social signals, among others, to support objective diagnosis. Finally, the development of tools to assist clinicians in the diagnosis and monitoring response to treatment and illness progression is gradually being supported by clinical studies <ref type="bibr" target="#b54">[57]</ref>. For the purpose of the present review, the term "depression assessment" will refer to the process of detecting and assessing the severity of signs of and/ or presence of depression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Inclusion Criteria</head><p>The technical report entitled "Procedures for Performing Systematic Reviews" by Kitchenham <ref type="bibr" target="#b55">[58]</ref> was used as a guide for the present review. The keywords used to search electronic databases and related resources are listed in <ref type="table" target="#tab_0">Table 1</ref>; the keywords were used interchangeably, in combinations of two or more, with either "OR" or "AND" operands. Inclusion criteria for the review involved a) adequate description of an algorithm for automatic depression assessment utilizing visual cues, and b) presentation of systematically derived data, producing concrete results. Strictly clinical studies, as well as approaches solely relying on speech-derived cues, were not included.</p><p>Publication dates of reviewed studies meeting criteria range between 2007 and April of 2017 (publications since 2005 were considered). <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the rapid increase of relevant studies during the past few years proving that automatic depression assessment based on visual cues is a rapidly growing research domain. The small drop in 2015 can be attributed to the fact that there was no Depression sub-challenge in the 2015 AVEC; similarly, the sharp rise of interest in 2013, 2014, and 2016 can be attributed to the respective AVEC challenges.</p><p>The current review focuses on the following study features: the research question addressed (detection/severity assessment/prediction), the number of modalities employed, facial signs, facial regions of interest (ROIs), number of participants (control/patients), technical specifications (image resolution/frame rate), experimental protocols for dataset acquisition, feature descriptors, fusion algorithms, decision methods, and scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Related Work</head><p>Despite the rising interest in the topic, existing reviews vary in their specific focus, and rarely attempt an in depth assessment of methods and results. For instance, in their report on health-enabling technologies for addiction and depression detection Jâ‚¬ ahne-raden et al. <ref type="bibr" target="#b56">[59]</ref> reviewed three studies addressing depression detection based on facial activity.</p><p>Valstar <ref type="bibr" target="#b53">[56]</ref> included a non-technical description of five relevant studies in his work on "Automatic Behavior Understanding in Medicine", under the mood and anxiety disorders section. D'Mello <ref type="bibr" target="#b57">[60]</ref> reviewed five publications relevant to depression assessment within the general context of mental state detection. Schuller <ref type="bibr" target="#b58">[61]</ref> discussed many topics of shared interest with the present review, as did Martinez and Valstar <ref type="bibr" target="#b59">[62]</ref>, but within the broader field of affective computing and facial image analysis, and not specifically focusing on depression assessment. Hyett et al. <ref type="bibr" target="#b60">[63]</ref> reviewed approaches for the detection of melancholia, including eight papers related to depression, and went into depth analyzing the utility of a chosen set of algorithms. The most extensive review to date by Girard and Cohn <ref type="bibr" target="#b35">[34]</ref> includes twenty-one studies, presenting the core concepts and providing the necessary information for someone who is getting acquainted with the field, without, in our view, expanding on many technical details. Cummins et al. <ref type="bibr" target="#b61">[64]</ref> conducted an extensive review of depression and suicide risk assessment, with a focus on speech analysis. Finally, the survey by Corneanu et al. <ref type="bibr" target="#b62">[65]</ref> is adequately thorough with respect to the algorithms used for facial expression recognition, but is limited to only ten applications to depression assessment.</p><p>In the current exhaustive review of more than sixty studies, technical details, potential limitations of each approach and classification accuracy achieved are evaluated, focusing on image processing and machine learning algorithms applied to depression detection. Additional modalities employed (speech, physiological signals, contextual information) and computational cost factors related to system requirements, e.g., camera resolution and frame rate, are also considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Structure of the Paper</head><p>The current review is organized in seven sections. Section 2 covers nonverbal assessment of depression and summarizes the visual signs identified in the reviewed studies. The relevant datasets used for evaluating systems for automatic depression assessment are described in Section 3, along with respective data collection procedures. Section 4 reviews image processing and machine learning algorithms used for automatic assessment of depression, while Section 5 presents a quantitative meta-analysis of selected studies. Discussion of the main review findings and implications for future studies are included in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NONVERBAL SIGNS FOR DEPRESSION ASSESSMENT</head><p>It is well known that depression manifests through a variety of nonverbal signs <ref type="bibr" target="#b63">[66]</ref>, <ref type="bibr" target="#b64">[67]</ref>. Involuntary changes in the tonic activity of facial muscles, as well as changes in peripheral blood pressure, and skin electrodermal response, often mirror the frequent and persistent negative thoughts and feelings of sadness that characterize depression. Preliminary findings suggest that electroencephalographic recordings may contain features related to depression <ref type="bibr" target="#b65">[68]</ref>. Functional Near-Infrared Spectroscopy (fNIRS) has also attracted interest <ref type="bibr" target="#b66">[69]</ref>, <ref type="bibr" target="#b67">[70]</ref>. Additionally, speech conveys non-verbal information on the mental state of the speaker; prosodic, source, and acoustic features, as well as vocal tract dynamics, are speech-related features affected by depression <ref type="bibr" target="#b61">[64]</ref>. Furthermore, depression as a mood disorder, is portrayed on the individual's appearance, in terms of facial expression, as well as body posture <ref type="bibr" target="#b63">[66]</ref>, <ref type="bibr" target="#b64">[67]</ref>. Face as a whole, and individual facial features, such as eyes, eyebrows or mouth, are of particular interest when it comes to depression assessment. Some of the visual signs identified in the reviewed papers are briefly described in the paragraphs that follow. A visual sign that has drawn considerable attention by clinicians in relation to depression assessment is pupil dilation. Siegle et al. <ref type="bibr" target="#b68">[71]</ref> reported faster pupillary responses in non-depressed individuals to positive rather than negative stimuli. In contrast, depressed persons displayed slower pupil dilation responses to positive stimuli in conditions associated with reduced cognitive load (see also <ref type="bibr" target="#b69">[72]</ref>, <ref type="bibr" target="#b70">[73]</ref>, <ref type="bibr" target="#b71">[74]</ref>, <ref type="bibr" target="#b72">[75]</ref>, <ref type="bibr" target="#b73">[76]</ref>, <ref type="bibr" target="#b74">[77]</ref>). More recently Price et al. <ref type="bibr" target="#b75">[78]</ref> investigated attentional bias, including pupil bias and diameter, to predict depression symptoms over a two year follow up period in a sample of adolescents displaying high ratings of anxiety. Saccadic eye movements have also been found to differ in terms of latency and duration between depressed and healthy participants <ref type="bibr" target="#b76">[79]</ref>, <ref type="bibr" target="#b77">[80]</ref>.</p><p>Action Units (AUs), introduced by Ekman et al. <ref type="bibr" target="#b78">[81]</ref>, were first utilized for automatic depression assessment by Cohn et al. <ref type="bibr" target="#b79">[82]</ref>. AUs have been studied in terms of frequency of occurrence, mean duration, onset/total duration, and onset/offset ratios. At approximately the same time McIntyre et al. <ref type="bibr" target="#b80">[83]</ref> proposed an AU-based approach in the form of Region Units (RUs). Several studies have reported promising results on the application of AUs to automatic depression assessment <ref type="bibr" target="#b81">[84]</ref>, <ref type="bibr" target="#b82">[85]</ref>, <ref type="bibr" target="#b83">[86]</ref>, <ref type="bibr" target="#b84">[87]</ref>, <ref type="bibr" target="#b85">[88]</ref>, <ref type="bibr" target="#b86">[89]</ref>, <ref type="bibr" target="#b87">[90]</ref>, <ref type="bibr" target="#b88">[91]</ref>, <ref type="bibr" target="#b89">[92]</ref>, <ref type="bibr" target="#b90">[93]</ref>, <ref type="bibr" target="#b91">[94]</ref>, <ref type="bibr" target="#b92">[95]</ref>, <ref type="bibr" target="#b93">[96]</ref>. Specific facial expressions, have also been examined for depression assessment, in terms of frequency of occurrence, variability, and intensity of a specific expression. Typically, the facial expression classification system proposed by Ekman <ref type="bibr" target="#b95">[97]</ref> is employed, which includes a set of six basic emotions (joy, surprise, anger, sadness, disgust, fear). Measuring the frequency of occurrence of each of the six emotional expressions <ref type="bibr" target="#b72">[75]</ref>, <ref type="bibr" target="#b81">[84]</ref>, <ref type="bibr" target="#b82">[85]</ref>, <ref type="bibr" target="#b83">[86]</ref>, <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b97">[99]</ref>, <ref type="bibr" target="#b98">[100]</ref> relies on the premise that depressed individuals tend to show reduced expressivity <ref type="bibr" target="#b63">[66]</ref>. Other studies focused on specific facial features, such as the eyes and mouth. These include gaze direction <ref type="bibr" target="#b98">[100]</ref>, <ref type="bibr" target="#b99">[101]</ref>, <ref type="bibr" target="#b100">[102]</ref>, <ref type="bibr" target="#b101">[103]</ref>, reduced eye contact <ref type="bibr" target="#b102">[104]</ref>, eyelid activity <ref type="bibr" target="#b103">[105]</ref>, eye openings/blinking <ref type="bibr" target="#b72">[75]</ref>, <ref type="bibr" target="#b104">[106]</ref>, <ref type="bibr" target="#b105">[107]</ref> and iris movement <ref type="bibr" target="#b104">[106]</ref>. Smile intensity <ref type="bibr" target="#b98">[100]</ref>, <ref type="bibr" target="#b99">[101]</ref>, <ref type="bibr" target="#b101">[103]</ref>, smile duration <ref type="bibr" target="#b99">[101]</ref>, <ref type="bibr" target="#b101">[103]</ref>, mouth animation <ref type="bibr" target="#b105">[107]</ref>, listening smiles (smiles while not speaking) <ref type="bibr" target="#b100">[102]</ref>, and lack of smiles <ref type="bibr" target="#b102">[104]</ref> also constitute potentially useful facial signs for assessing depression.</p><p>Head pose, orientation, and movement have been used extensively for depression assessment <ref type="bibr" target="#b72">[75]</ref>, <ref type="bibr" target="#b81">[84]</ref>, <ref type="bibr" target="#b82">[85]</ref>, <ref type="bibr" target="#b83">[86]</ref>, <ref type="bibr" target="#b87">[90]</ref>, <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b97">[99]</ref>, <ref type="bibr" target="#b98">[100]</ref>, <ref type="bibr" target="#b99">[101]</ref>, <ref type="bibr" target="#b101">[103]</ref>, <ref type="bibr" target="#b104">[106]</ref>, <ref type="bibr" target="#b105">[107]</ref>, <ref type="bibr" target="#b106">[108]</ref>, <ref type="bibr" target="#b107">[109]</ref>, <ref type="bibr" target="#b108">[110]</ref>, <ref type="bibr" target="#b109">[111]</ref>, along with motor variability and general facial animation <ref type="bibr" target="#b81">[84]</ref>, <ref type="bibr" target="#b83">[86]</ref>, <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b105">[107]</ref>. Additionally, body gestures <ref type="bibr" target="#b107">[109]</ref>, <ref type="bibr" target="#b108">[110]</ref>, <ref type="bibr" target="#b109">[111]</ref>, <ref type="bibr" target="#b110">[112]</ref>, <ref type="bibr" target="#b111">[113]</ref> involving the entire body, the upper body, or separate body parts can also contribute to the assessment. Finally, shaking and/or fidgeting behavior, self-adaptors, and foot tapping <ref type="bibr" target="#b100">[102]</ref> have also been considered as signs of depression. <ref type="table" target="#tab_1">Table 2</ref>, inspired by <ref type="bibr" target="#b61">[64]</ref>, <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b101">[103]</ref>, <ref type="bibr" target="#b112">[114]</ref> and enhanced hereby, is summarizing all of the signs and signals related to depression assessment as found in the literature to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEPRESSION DATASETS</head><p>The availability of empirical data is of paramount importance for the evaluation of methods for automatic assessment of depression. Such data are critical during algorithm development and testing. Due to the sensitive nature of clinical data, availability is neither wide nor free, and this is the reason that most research groups resort to generating their own data sets. This section describes procedures used for data collection and derived datasets found in the reviewed studies (cf. <ref type="table" target="#tab_2">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection Procedure</head><p>Recruitment of participants is perhaps the most challenging step in this line of research. Patients with MDD were recruited from the community, in many cases by clinical psychologists or social workers, and were assessed using DSM-IV <ref type="bibr" target="#b113">[115]</ref> criteria <ref type="bibr" target="#b68">[71]</ref>, <ref type="bibr" target="#b69">[72]</ref>, <ref type="bibr" target="#b71">[74]</ref>, <ref type="bibr" target="#b79">[82]</ref>, <ref type="bibr" target="#b87">[90]</ref>, <ref type="bibr" target="#b109">[111]</ref> and/or HAM-D scores <ref type="bibr" target="#b79">[82]</ref>, <ref type="bibr" target="#b88">[91]</ref>, <ref type="bibr" target="#b109">[111]</ref>, <ref type="bibr" target="#b111">[113]</ref>; patients may be medicated, un-medicated or in remission. The Mini International Neuropsychiatric Interview (MINI) was employed in the data collection for the dataset reported in <ref type="bibr" target="#b114">[116]</ref> in order to obtain the diagnosis, and Quick Inventory of Depressive Symptomatology-Self Report (QIDS-SR) for defining the severity. BDI has also been used in <ref type="bibr" target="#b68">[71]</ref> to establish whether a given patient was in remission. Comparison data were obtained from individuals who had never been diagnosed with depression or other mood disorder. Data collection from non-clinical samples, employed Self-RIs such as PHQ-9 <ref type="bibr" target="#b81">[84]</ref>, <ref type="bibr" target="#b82">[85]</ref>, <ref type="bibr" target="#b83">[86]</ref>, <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b99">[101]</ref>, <ref type="bibr" target="#b100">[102]</ref> and BDI <ref type="bibr" target="#b48">[51]</ref>, <ref type="bibr" target="#b50">[53]</ref>, assessing the severity of (sub-clinical) depression-related symptomatology. Recruitment methods further included flyers, posters, institutional mailing lists, social networks, and personal contacts.</p><p>In order to ensure that the collected data carry useful information, the experimental protocol must be carefully designed. Information on participant characteristics includes cognitive abilities, assessed mainly through executive function tests, and psychoemotional traits-assessed through self-report questionnaires. Across studies, executive tasks include sorting <ref type="bibr" target="#b68">[71]</ref>, planning, and problem solving tasks <ref type="bibr" target="#b115">[117]</ref>. For instance, the dataset constructed for the AVEC'13 integrated a series of "activation tasks", including vowel pronunciation, solving a task out loud, counting from 1 to 10, reading novel excerpts, singing, and describing a specific scene displayed in pictorial form <ref type="bibr" target="#b48">[51]</ref>.</p><p>Establishing conditions which enable the collection of signs related to depression is by far the most important step, as also discussed in <ref type="bibr" target="#b61">[64]</ref>. Methods employed can vary significantly across studies. Emotion elicitation is used to measure reactions to emotionally charged stimuli, given that such reactions significantly differ between healthy and patient groups. The Handbook of Emotion Elicitation and Assessment <ref type="bibr" target="#b116">[118]</ref> describes several methods for eliciting emotion in the laboratory including: emotional film clips used in <ref type="bibr" target="#b80">[83]</ref>, <ref type="bibr" target="#b107">[109]</ref>, images selected from the International Affective Picture System (IAPS) used in <ref type="bibr" target="#b80">[83]</ref>, <ref type="bibr" target="#b107">[109]</ref>, social psychological methods, and dyadic interaction. Emotionally charged images and clips are in principle capable of eliciting observable responses, although ethical considerations set limits to the shocking nature of the content. Social experiments also raise certain ethical issues and may not be suitable for emotionally vulnerable persons. In this regard it is imperative that patients with depression are not subjected to unnecessary and unwanted stress or anxiety.</p><p>Dyadic interaction is an appealing method as it involves social context, and affords a wide range of elicited emotions <ref type="bibr" target="#b61">[64]</ref>. Contemporary models of emotional expression propose that the intensity of relevant signs is proportional to the degree of sociality of the eliciting situation, ranging from being physically in the same room with another person, communicating over the phone, to simply thinking of the other person <ref type="bibr" target="#b117">[119]</ref>, <ref type="bibr" target="#b118">[120]</ref>. The suitability of dyadic interaction contexts is further supported by the notion that many of the behavioral indicators of depression are asocial in nature <ref type="bibr" target="#b35">[34]</ref>. A shortcoming of this method lies in its unstructured nature, which does not guarantee that the targeted social or emotional responses will actually be produced.</p><p>Structured Interviews are usually employed for gathering depressive symptoms, but have also been used for eliciting specific emotions by asking participants to describe personal, emotionally charged events <ref type="bibr" target="#b80">[83]</ref>. Interviews can take place over one or more sessions, conducted by a therapist or a virtual character (VC), or guided by instructions on a computer screen. Typically the interview topic changes smoothly from casual to more intimate topics <ref type="bibr" target="#b79">[82]</ref>, <ref type="bibr" target="#b80">[83]</ref>, <ref type="bibr" target="#b81">[84]</ref>, <ref type="bibr" target="#b83">[86]</ref>, <ref type="bibr" target="#b84">[87]</ref>, <ref type="bibr" target="#b87">[90]</ref>, <ref type="bibr" target="#b88">[91]</ref>, <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b97">[99]</ref>, <ref type="bibr" target="#b99">[101]</ref>, <ref type="bibr" target="#b101">[103]</ref>, <ref type="bibr" target="#b107">[109]</ref>, <ref type="bibr" target="#b115">[117]</ref>.</p><p>The amount of visual data that is necessary for a reliable assessment depends heavily upon the temporal nature of MDD. The specificity of the assessment method may benefit from multiple recording sessions, such as that of the data reported in <ref type="bibr" target="#b79">[82]</ref>, <ref type="bibr" target="#b87">[90]</ref>, <ref type="bibr" target="#b88">[91]</ref>. Recording length depends on the elicitation method, with structured interviews being considerably longer in comparison with recordings based on emotion elicitation through films.</p><p>Additional modalities, such as speech <ref type="bibr" target="#b79">[82]</ref>, <ref type="bibr" target="#b82">[85]</ref>, <ref type="bibr" target="#b90">[93]</ref>, <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b97">[99]</ref>, <ref type="bibr" target="#b98">[100]</ref>, <ref type="bibr" target="#b99">[101]</ref>  <ref type="bibr" target="#b100">[102]</ref>, <ref type="bibr" target="#b101">[103]</ref>, <ref type="bibr" target="#b105">[107]</ref>, <ref type="bibr" target="#b107">[109]</ref>, <ref type="bibr" target="#b108">[110]</ref>, <ref type="bibr" target="#b109">[111]</ref>, <ref type="bibr" target="#b119">[121]</ref>, <ref type="bibr" target="#b120">[122]</ref>, <ref type="bibr" target="#b121">[123]</ref>, <ref type="bibr" target="#b122">[124]</ref>, <ref type="bibr" target="#b123">[125]</ref>, <ref type="bibr" target="#b124">[126]</ref>, <ref type="bibr" target="#b125">[127]</ref>, <ref type="bibr" target="#b126">[128]</ref>, <ref type="bibr" target="#b127">[129]</ref>, physiological signals <ref type="bibr" target="#b100">[102]</ref>, <ref type="bibr" target="#b128">[130]</ref>, <ref type="bibr" target="#b129">[131]</ref>, and written text <ref type="bibr" target="#b82">[85]</ref>, <ref type="bibr" target="#b97">[99]</ref> have been employed in order to enhance assessment sensitivity. Consequently, studies vary widely depending on the types of equipment utilized, the different modalities and particular signs monitored. For instance, studies focusing on the pupillary response may only use a pupilometer <ref type="bibr" target="#b68">[71]</ref>, <ref type="bibr" target="#b69">[72]</ref>, <ref type="bibr" target="#b70">[73]</ref>, <ref type="bibr" target="#b71">[74]</ref> and pay special attention to ambient illumination in order to optimize sensitivity. Again, depending on the approach, one or more cameras, typically color, are simultaneously used to cover more than one viewing angles and fields of view (e.g., both face and body separately <ref type="bibr" target="#b79">[82]</ref>). Thermal imaging has  <ref type="bibr" target="#b130">[132]</ref>, as well as in studies focusing on saccadic eye movements, employing an infrared eye tracking system <ref type="bibr" target="#b76">[79]</ref>, <ref type="bibr" target="#b77">[80]</ref>. Depth sensors (e.g., Microsoft Kinect) have also been utilized in some cases <ref type="bibr" target="#b81">[84]</ref>, <ref type="bibr" target="#b99">[101]</ref>. Microphones are naturally required for recording the participants' speech during an interview or narrating tasks. Again, as with emotion elicitation, the precise setup varies across studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reported Datasets</head><p>The various datasets reported in relevant work are summarized in <ref type="table" target="#tab_2">Table 3</ref>. Participant demographics, stimuli, ground truth, selection criteria, research question, as well as technical specifications, are some of the features that vary across studies. Most of the studies employed adult participants, while two recruited adolescents. Methods for collecting depressive symptomatology included: a) interpersonal, i.e., interview with a clinical psychologist or interaction with family members, b) non-social, where participants were presented with stimuli on a computer, and c) combination of (a) and (b). The ground truth for the presence of depression varied accordingly, relying on clinical assessment in the majority of the cases, and on self-reports in two of the studies. The selection criteria used depended greatly on the research question. DSM and HAM-D criteria were used for detection of depression <ref type="bibr" target="#b79">[82]</ref>, <ref type="bibr" target="#b87">[90]</ref>, <ref type="bibr" target="#b108">[110]</ref>, <ref type="bibr" target="#b109">[111]</ref>, <ref type="bibr" target="#b111">[113]</ref>, <ref type="bibr" target="#b130">[132]</ref> or differentiation from Bipolar Disorder <ref type="bibr" target="#b92">[95]</ref>. Others had more specific criteria, i.e., patients recovering from Deep Brain Stimulation of the Subcallosal Cingulate Cortex (DBS-SCC) <ref type="bibr" target="#b131">[133]</ref>, in order to monitor recovery progress. Studies assessing the predictive value of the method for future emergence of clinical depression in adolescents involved 9-12 year old participants at the initial data collection, with clinical reassessment after a two year interval <ref type="bibr" target="#b115">[117]</ref>, <ref type="bibr" target="#b123">[125]</ref>.</p><p>Technical specifications of the video recording equipment varied to some extent, but not significantly, as the setup typically involved a single camera monitoring the participants' face/upper body. A notable exception was the setup employed for the Pittsburgh dataset, utilizing four hardware-synchronized analogue cameras; two for monitoring the participant's head and shoulders, one for full body monitoring, and the fourth monitoring the interviewer, together with two microphones for speech recording.</p><p>Regarding dataset availability, AVEC is the only fully available dataset for free download, 2 while the DAIC-WOZ dataset (Distress Analysis Interview Corpus-Wizard of Oz) is also partly available. <ref type="bibr" target="#b3">3</ref> Both datasets require a signed End User License Agreement (EULA) in order to provide download access. Pittsburgh dataset is also expected to release features by June 2017. <ref type="bibr" target="#b4">4</ref> The remaining reported datasets are proprietary, while in some cases they have been made available to visiting researchers. The number of participants listed in <ref type="table" target="#tab_2">Table 3</ref> is that reported in the latest publication employing the related dataset. However, different published papers report results obtained from different subsets; accordingly, sample size used in each published report is specified in Section 5 <ref type="table" target="#tab_6">(Tables 7 and 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AUTOMATIC ASSESSMENT OF DEPRESSION</head><p>The generic processing flow of an automatic system for depression assessment, combining the standard structure for automated audiovisual behavior analysis proposed by Girard and Cohn <ref type="bibr" target="#b35">[34]</ref>, with fusion methods presented in Alghowinem's thesis <ref type="bibr" target="#b133">[134]</ref>, is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. Given a visual input (image sequence), along with other types of signals, such as audio, text from transcripts, and physiological signals, the prerequisite step is that of preprocessing. Feature extraction algorithms are subsequently applied to all visual signals, as described in  2. http://avec2013-db.sspnet.eu/ 3. http://dcapswoz.ict.usc.edu/ 4. http://www.pitt.edu/ emotion/depression.html suitable for categorical assessment, such as discriminating between a given number of classes (i.e., depressed versus not depressed or low versus high depression severity). For continuous assessment of depression (e.g., depression severity according to BDI scores ranging between 0-63) regression analyses are more appropriate. Fusion from different feature sets, either visual or from different modalities, can also take place at the decision phase (more details are given in Section 4.3.4). Methods used in the reviewed studies (i.e., algorithms for feature extraction from visual signs, feature selection algorithms, decision methods, fusion techniques) are described in turn in the following section. Given that the present systematic review is focused on depression assessment, a more extensive description of relevant methods can be found in Corneanu et al. <ref type="bibr" target="#b62">[65]</ref> with respect to algorithms for facial expression recognition in generally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preprocessing</head><p>Given a visual input (video), illumination normalization, registration and alignment between the image sequences, and face detection are typical required preprocessing steps. Other types of signals, such as speech or physiological recordings, may also need preprocessing, such as segmentation. The most popular algorithm for face detection has been proposed by Viola and Jones <ref type="bibr" target="#b155">[153]</ref>. Some off-the-shelf facial expression analysis applications have also been used widely as preprocessing tools, enabling researchers to focus on deriving high level information. An example of such a tool is the OpenFace freeware application <ref type="bibr" target="#b5">5</ref>  <ref type="bibr" target="#b156">[154]</ref>. SEMAINE API, an open source framework for building emotion-oriented systems, is another potentially useful preprocessing tool 5. https://www.cl.cam.ac.uk/ tb346/res/openface.html <ref type="bibr" target="#b157">[155]</ref>. The Computer Expression Recognition Toolbox (CERT) <ref type="bibr" target="#b158">[156]</ref> has been quite popular, but has now become commercialized. Tools for gaze estimation, such as the one presented in <ref type="bibr" target="#b159">[157]</ref>, <ref type="bibr" target="#b160">[158]</ref>, may help derive important features relevant to signs of depression, such as fixation or shorter eye-contact. Z-Face <ref type="bibr" target="#b161">[159]</ref> has also been employed for alignment and landmark detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Extraction / Manipulation</head><p>This section describes processes involved in feature extraction, dimensionality reduction, and fusion. The output of this processing stage generates the input to the machine learning stage, where no further manipulation of features is taking place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Feature Extraction</head><p>Feature extraction is an important step in the processing workflow, since subsequent steps entirely depend on it. The approaches reviewed employ a wide range of feature extraction algorithms which, according to the well-established taxonomy in <ref type="bibr" target="#b62">[65]</ref>, can be classified as a) geometry-based, or b) appearance-based. In the field of depression assessment, several features are derived from the time-series of both (a) and (b) in the form of dynamic features. Close inspection of depression manifestations, listed in <ref type="table" target="#tab_1">Table 2</ref>, reveals that the majority of signs involve muscle activity, which accounts for the temporal nature of the features. Features can be further categorized as high or low level; high level features directly translate to human common sense, while low level features are based on "traditional" image processing descriptors. Depending on the approach, the software packages mentioned in Preprocessing could also serve as feature extraction methods (e.g., OpenFace, SEMAINE API, CERT, etc). In the present paper feature extraction algorithms are grouped into those focusing on the face region and those relying on the body region. A pictorial taxonomy of the various algorithms, including the region of interest on which they are applied, the features computed, and references to respective studies, is presented in <ref type="figure" target="#fig_3">Fig. 3</ref>. Features associated with statistically significant differences between study and control groups, as reported in the original papers, are presented in <ref type="table" target="#tab_3">Table 4</ref>. The various features, retrieved from relevant studies, are described below in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face</head><p>Features related to the face are classified here into features from full face, AUs, facial landmarks, and mouth/eyes.</p><p>Full Face. As it becomes apparent from <ref type="figure" target="#fig_3">Fig. 3</ref>, approaches employing feature extraction from the entire face region comprise by far the most popular category. Certain high level features extracted from the face as a whole concern basic emotional expressions displayed, given that depression is associated with reduced variability of emotional expression and greater persistence of a neutral expression. Heart-rate, derived unobtrusively from facial images, has also been used as a feature for detecting depression.</p><p>As expected, geometrical features, such as edges, corners, coordinates, and orientation, are often used to represent facial expressions. Functionals derived from the time series of geometric features are quite popular. Some examples are average, minimum, and maximum values of displacements, velocities, or accelerations of the coordinates that define the face region as a whole. Functionals from roll, pitch, and yaw, have also been employed in some approaches, along with the frequency of certain rotation angles. Other approaches go one step further, to compute functionals from the time series of feature points and eigenvectors, rather than relying on simple coordinates. Eigenvalues have also been used for the computation of multi-scale entropy.</p><p>In one case the difference of face co-ordinates between the first and the last frame was considered.</p><p>Appearance-based algorithms are also very popular for full-face based features. Among the most prevalent texture descriptors are Local Binary Patterns (LBP). For LBP to be computed, the image is divided into partially overlapping windows and each pixel of the window is compared to its neighbors producing a binary value; histograms are then constructed on the occurrences of these values, and all histograms are concatenated to form the feature vector. Several variants of LBPs have been created for automatic depression assessment, such as an extension of LBP that considers patterns on Three Orthogonal Planes (LBP-TOP): XY, XT and YT; XY represents each frame, while XT and YT encode space-time information. Local Gabor Binary Patterns-Three Orthogonal Planes (LGBP-TOP) extends LBP-TOP by computing patterns on the output of Gabor-filtered data, rather than on the original intensity image. Gabor filters have been shown to share many similarities with properties of the human visual system. Along the same lines, Local Curvelet Binary Patterns-Three Orthogonal Planes (LCBP-TOP) was introduced in some studies, which entails computing the patterns on the curvelet transform of the original image. Local Curvelet Binary Patterns-Pairwise Orthogonal Planes (LCBP-POP) is yet another variation of the algorithm operating on pairs of orthogonal planes. Additionally, the Block-Wise LBP-TOP (BW-LBP-TOP) method which computes the LBP-TOP for a specific number of nonoverlapping blocks, has also been employed.</p><p>Local Phase Quantization (LPQ) is another texture descriptor computed on the quantized Fourier phase in order to produce the binary coding for each pixel; it has been shown that the derived descriptor is immune to moderate blurring. An extension of LPQ is Local Phase Quantization-Three Orthogonal Planes (LPQ-TOP), based on the same concept as LBP-TOP. Eigenfaces, which applies eigenvector decomposition to facial images and face recognition, and Fisherfaces representing faces on a subspace through Linear Discriminant Analysis (LDA), have also been used for automatic depression assessment. Another popular algorithm for motion-based approaches is the histogram of optical flow, which estimates the motion within visual representations, by using vectors for each pixel in order to describe a dense motion field. Divergence-Curl-Shear (DCS) descriptors are also based on optical flow, whereby optical flow vectors are transformed into the motion features of divergence, curl and shear.</p><p>The Motion History Histograms (MHH) algorithm, which extends Motion History Images (MHI), has also been found in the related literature. MHI is a grayscale image representing local motion: white pixels for latest motion, darkest gray intensities for the earliest, and lighter gray values for intermediate latencies. MHH extends MHI by considering patterns of movement across frame series. A further extension of MHH is the 1-D MHH, which is computed on the feature vector sequence, instead of the intensity image. The Difference Image is a simplified process, which considers intensity differences between the first and the last frame. A similar approach considers pixel differences for every two successive frames, and also the variance and quantiles of the average pixel differences between every two frames. Finally, the Space-Time Interest Points (STIP) algorithm, which detects local structures characterized by significant intensity variability in both space and time, has also been used in several reported studies.</p><p>Facial Landmarks. Facial landmarks have been very popular in addressing problems related to facial expression analysis, and have been applied to depression assessment. Such algorithms localize fiducial points of the face and facial features, which are very useful in extracting high level traits directly associated with signs of depression, e.g., smiling. The Constraint Local Models method (CLM) introduced by Saragih et al. <ref type="bibr" target="#b162">[160]</ref> is displayed in <ref type="figure" target="#fig_4">Fig. 4</ref> to illustrate its application to the modeling of facial geometry. Active Shape Models (ASM) as well as Active Appearance Models (AAMs) have also been utilized for depression assessment methods. Other modelbased approaches, used in the reported studies, include 3D Landmark Model Matching, Elastic Bunch Graph Matching (EBFM), and Landmark Distribution Model (LDM).</p><p>In addition, facial landmark data have also been analyzed as time series. Displacement, velocity, acceleration, as well as the landmark coordinates alone, have been used as features. Furthermore, displacement of each landmark from the mid-horizontal axis, landmark velocity and acceleration have been used as motion-related features. Additionally, velocity vectors of features that represent the relative position of lower-level landmarks have been utilized in some studies, such as the mean distance of upper to lower eyelid landmarks, and mean squared distance of all mouth landmarks to the mouth centroid. In addition, polynomial fitting, as well as statistics derived from shape eigenvector velocities have been utilized. Landmark Motion History Images (LMHI) is a low level feature which, instead of the actual intensities, computes the MHI on the motion of the facial landmarks. LMHI has been combined with Histogram of Oriented Gradients (HOG), as well as with LBP. Finally, the Landmark Motion Magnitude (LMM) algorithm has also been applied to the vectors which displace each landmark from one frame to the next. Action Units. AUs encode the coordinated activity of groups of facial muscles in correspondence to specific actions, including specific emotional expressions. They can be employed for measuring the Variability of Facial Expression (VFE), as depressed individuals tend to be less expressive. Other approaches apply AUs as high-level features. AU occurrence by itself is meaningful as there are specific facial actions that are directly linked to the presence of depression (smiling, mouth corners angled down, etc.). Examples of AUs related to affective states common in depression, such as sadness and distress, according to Emotional Facial Action Coding System (EMFACS) <ref type="bibr" target="#b163">[161]</ref> are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. It should be clarified, however, that there are AUs which do not necessarily occur as a result of affect-related events, but are associated with non-affective orofacial movements, such as speech and chewing. Additionally, although several approaches implement AUs dynamically (e.g., duration, base rate, ratio of onset/offset), AUs are essentially static signs.</p><p>Mouth &amp; Eyes. Apart from the face as a whole, features extracted individually from the mouth and eyes have also been found in the reviewed literature. Smile intensity and duration is a mouth-based feature which has been employed for automatic depression assessment, consistent with the clinical literature, as depressed individuals tend to smile less often.</p><p>For the eye region, average vertical gaze, blinking rate, and pupil dilation have been reported. Pupillary response data from pupil radius measurements have been extracted through deformable template matching, which determines the pupil radius by using a pupil model formed by two concentric circles. Additionally, functionals from velocity and acceleration of horizontal and vertical eyelid movement have been used.</p><p>Features derived from thermal imaging, such as mean eye temperature, have also been used to differentiate depressed from healthy control samples. Additional features include saccade latency, peak velocity of initial saccade, saccade duration, mean and standard deviation (SD) of intersaccadic intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Body</head><p>Although body signs in general have been shown to convey manifestations of depression, few approaches have exploited their utility. Existing applications can be classified as relying on either upper body or relative body part movements. Features for upper body movements have been extracted through the STIP and DCS algorithms. Relative body part movements, on the other hand, have been exploited via the parts algorithm that represents orientation and distance from the torso center expressed in polar coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Dimensionality Reduction</head><p>Many feature extraction algorithms produce vectors of high dimensionality. The goal of dimensionality reduction algorithms is to reduce the number of features in a meaningful manner, in order to avoid corrupting the classifier. Dimensionality reduction algorithms can be classified in two groups: (a) Feature Transformation, and (b) Feature Selection <ref type="bibr" target="#b164">[162]</ref>. In the first group features are transformed/combined by being projected from a high-dimensional space to low-dimensional space, to increase separability. On the other hand, in the second group, as the name implies, a selection procedure takes place, and the most discriminative/significant features are selected. Below, examples from both groups, as retrieved in reported approaches, are being described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Transformation</head><p>Principal Components Analysis (PCA) is the most popular algorithm in this category <ref type="bibr" target="#b79">[82]</ref>, <ref type="bibr" target="#b90">[93]</ref>, <ref type="bibr" target="#b122">[124]</ref>, <ref type="bibr" target="#b127">[129]</ref>, <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b142">[142]</ref>, <ref type="bibr" target="#b148">[148]</ref>, <ref type="bibr" target="#b165">[163]</ref> and has been used to generate new features based on a linear transformation of the original features. Manifold learning with Laplacian Eigenmaps <ref type="bibr" target="#b87">[90]</ref>, <ref type="bibr" target="#b88">[91]</ref> supports non-linear dimensionality reduction, based on local computations by solving a sparse eigenvalue problem.</p><p>Another set of approaches for reducing dimensionality involves codebooks. Bag-of-Words (BoW) <ref type="bibr" target="#b108">[110]</ref>, <ref type="bibr" target="#b109">[111]</ref>, <ref type="bibr" target="#b110">[112]</ref>, <ref type="bibr" target="#b120">[122]</ref>, <ref type="bibr" target="#b121">[123]</ref>, <ref type="bibr" target="#b126">[128]</ref>, <ref type="bibr" target="#b139">[140]</ref>, <ref type="bibr" target="#b141">[141]</ref>, <ref type="bibr" target="#b148">[148]</ref>, initially intended for document classification, has been applied to image processing problems by treating individual features as words and creating a dictionary of image features. The Vector of Local Aggregated Descriptors (VLAD) <ref type="bibr" target="#b148">[148]</ref> also relies on codebook representation. Another dictionary based method is K-SVD (Singular Value Decomposition) <ref type="bibr" target="#b147">[147]</ref>.</p><p>Gaussian Mixture Models (GMM) <ref type="bibr" target="#b103">[105]</ref>, <ref type="bibr" target="#b106">[108]</ref> and Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b145">[145]</ref> have been adopted for classification and regression, respectively. In <ref type="bibr" target="#b115">[117]</ref>, <ref type="bibr" target="#b123">[125]</ref> PCA was used in combination with LDA as a feature extraction method, while providing the option of reducing dimensions of the feature vector. A histogrambased approach was presented in <ref type="bibr" target="#b122">[124]</ref> which entails maintaining the highest-scoring bins based on a predefined threshold adjusted to the total samples size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Selection</head><p>In <ref type="bibr" target="#b104">[106]</ref>, <ref type="bibr" target="#b114">[116]</ref>, and <ref type="bibr" target="#b82">[85]</ref> distributional statistics (e.g., t-tests) were employed to select only those features that met a predefined statistical threshold. In <ref type="bibr" target="#b135">[136]</ref> the authors implemented the minimum Redundancy Maximum Relevance (mRMR) feature selection, which considers statistical dependency between features.</p><p>Several feature selection approaches were evaluated in <ref type="bibr" target="#b105">[107]</ref>: supervised feature selection, brute-force selection, and a backward selection scheme using bivariate correlations. Min-Redundancy Max-Relevance (mRMR) <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b166">[164]</ref>, greedy forward feature selection <ref type="bibr" target="#b81">[84]</ref>, <ref type="bibr" target="#b83">[86]</ref>, relief from WEKA <ref type="bibr" target="#b6">6</ref>  <ref type="bibr" target="#b134">[135]</ref>, Mutual Information Maximization (MIM) 6. http://www.cs.waikato.ac.nz/ml/weka/ <ref type="bibr" target="#b152">[151]</ref>, are additional algorithms used for feature selection in the reviewed studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Feature Fusion</head><p>Several approaches involve a variety of features derived from different modalities (e.g., visual, audio, text), as well as within the same modality (e.g., visual from different body parts). Fusion methods are usually employed in order to combine multiple feature sets. More often fusion takes place immediately after feature extraction <ref type="bibr" target="#b82">[85]</ref>, <ref type="bibr" target="#b99">[101]</ref>, <ref type="bibr" target="#b108">[110]</ref>, <ref type="bibr" target="#b120">[122]</ref>, <ref type="bibr" target="#b121">[123]</ref>, <ref type="bibr" target="#b122">[124]</ref>, <ref type="bibr" target="#b124">[126]</ref>, <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b142">[142]</ref>, where the extracted feature vectors of the different modalities are concatenated. This concatenation (cf. <ref type="figure" target="#fig_2">Fig. 2</ref>) can take place before dimensionality reduction (i.e., the concatenated feature set is examined for the dimensionality reduction) or after (i.e., features sets are considered individually for dimensionality reduction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Machine Learning</head><p>The next step following feature extraction and manipulation, in all methods reviewed, is the machine learning stage. Depending on the particular research goals, different types of decision methods may be applied. Classification methods are appropriate to address categorical questions (e.g., "depressed" versus "non-depressed" and low versus high depression severity). When the research question concerns the concurrent prediction of depression severity through video-derived indices in a continuous manner, regression approaches are predominantly employed. Cross validation methods are typically applied before classification/regression step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Cross Validation Methods</head><p>Cross validation methods are employed to establish the algorithm reliability, namely its capacity to generalize well with newly introduced data. To establish reliability, a given data set is divided in two parts, one used to train the proposed algorithm, and another (left-out) to test its performance. Specific procedures used for dataset splitting include the leave-one-out <ref type="bibr" target="#b79">[82]</ref>, <ref type="bibr" target="#b82">[85]</ref>, <ref type="bibr" target="#b87">[90]</ref>, <ref type="bibr" target="#b88">[91]</ref>, <ref type="bibr" target="#b101">[103]</ref>, <ref type="bibr" target="#b103">[105]</ref>, <ref type="bibr" target="#b106">[108]</ref>, <ref type="bibr" target="#b121">[123]</ref>, <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b150">[150]</ref> and the k-fold method <ref type="bibr" target="#b97">[99]</ref>, <ref type="bibr" target="#b114">[116]</ref>, <ref type="bibr" target="#b144">[144]</ref>, <ref type="bibr" target="#b166">[164]</ref>. In the leave-one-out procedure, for a dataset of N samples, N training sets are created of size N-1, each time consisting of all but one sample. The algorithm is then tested N times on its capacity to classify the "left-out" cases for each set. Samples could be several for one subject, and therefore the leave-one-out could also be implemented in a leave-one-subject-out manner, where all samples from a specific subject are excluded each time. In the k-fold procedure the dataset is randomly split into k partitions, with one partition kept each time for testing and the remaining used for training the algorithm. This procedure is repeated for k times. In the context of the AVEC challenges, partitioning of the dataset into training and test sections was performed by the organizers, to permit direct comparisons between the algorithms used by participating groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Classification</head><p>An exhaustive list of classifiers employed in the reviewed studies can be found in <ref type="table" target="#tab_4">Table 5</ref> and were ranked in <ref type="figure" target="#fig_6">Fig. 6</ref>. Support Vector Machines (SVM) is by far the most popular method for categorical assessment of depression. This can be justified by the fact that SVMs are well suited for binary problems of high dimensionality <ref type="bibr" target="#b167">[165]</ref>, such as the distinction of low symptom severity/absent depression from high symptom severity/present depression. In addition, SVMs are suitable for non-linear combinations of kernel functions <ref type="bibr" target="#b168">[166]</ref>. Furthermore, a variety of Neural Networks have been employed, including Feed Forward Neural Networks, Probabilistic Neural Networks, Restricted Boltzmann Machines (RBM), Radial-basis functions, Multilayer Perceptron, and Extreme Learning Machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Regression</head><p>The continuous nature of depressive symptomatology is well supported by the clinical literature, as discussed in Section 1.1. As a result, relevant approaches have recently been gaining momentum, including the AVEC challenges aimed at predicting scores on self-report depression scales as a continuous variable using speech and video cues. A common underlying objective in these methods is to develop a function through a combination of features, which can then be used to compute predicted depression severity scores for each participant. As it can be observed in <ref type="table" target="#tab_5">Table 6</ref>, the most popular regression algorithm, similarly to the classification-based approaches, is  Support Vector Regression. Again the ranking of algorithms is illustrated in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Decision Fusion</head><p>Fusion at the decision level involves combining different classification/regression results (cf. <ref type="figure" target="#fig_2">Fig. 2)</ref>. The results to be combined can be decisions based on a single modality, but also from feature level fusion, in every possible combination. Fusion for regression-based algorithms is regarded as more challenging due to the multicollinearity which characterizes observational datasets, such as the data used for depression assessment.</p><p>Logical operations are the simplest way to implement decision fusion for binary classification (such as for high/ low depression severity or presence/absence of depression). AND and OR operators have often been used to combine classification decisions <ref type="bibr" target="#b93">[96]</ref>, <ref type="bibr" target="#b108">[110]</ref>, <ref type="bibr" target="#b111">[113]</ref>, <ref type="bibr" target="#b121">[123]</ref>, <ref type="bibr" target="#b150">[150]</ref>. Training an SVM classifier on scores representing distance from the SVM classifier is another often used approach for decision fusion <ref type="bibr" target="#b108">[110]</ref>, <ref type="bibr" target="#b121">[123]</ref>. Another approach considered SVR output from individual modalities as input to a nextlevel regressor <ref type="bibr" target="#b146">[146]</ref>.</p><p>Generalized Linear Models <ref type="bibr" target="#b127">[129]</ref> and Expectation Maximization algorithms <ref type="bibr" target="#b82">[85]</ref> have been applied to decision-level fusion as special cases of regression algorithms. Kalman filtering has also been used to predict the most likely decision <ref type="bibr" target="#b125">[127]</ref>. In <ref type="bibr" target="#b119">[121]</ref> decision fusion was implemented linearly through a weighted sum according to the formula</p><formula xml:id="formula_0">D linear Ã°xÃž Â¼ X K iÂ¼1 w i D i Ã°xÃž:<label>(1)</label></formula><p>Wherex is the test sample, D i Ã°xÃž is the ith decision, and w i the corresponding weight Ã° P K iÂ¼1 w i Â¼ 1Ãž. Williamson et al. <ref type="bibr" target="#b90">[93]</ref>, <ref type="bibr" target="#b169">[167]</ref> also computed a weighted sum according to</p><formula xml:id="formula_1">w i Â¼ R 2 i =Ã°1=R 2 i Ãž;<label>(2)</label></formula><p>where R is the correlation between predicted and actual BDI scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SELECTED APPROACHES &amp; META-ANALYSIS</head><p>In this section different approaches, either classification-or regression-based, are compared in a quantitative manner.</p><p>The aim is to derive meaningful conclusions regarding the state of the field, and provide a means to identify the most appropriate setups, both in terms of data collection and algorithms employed. To be included in the analysis, studies must have reported results on automatic assessment of depression using visual features. Deciding on what is the 'best' approach for potential clinical use is beyond the scope of this systematic review. 'Winning' approaches can only be declared in challenges, where conditions are comparable and strictly defined. This is not the case for the various approaches included in this analysis and summarized in <ref type="table" target="#tab_6">Tables 7, 8</ref>, and 9, since they were typically evaluated on different datasets (or subsets from the same corpus of data). Even in the case of AVEC participations, direct comparisons across the three challenges are not possible given that different data sets were used in each. The specific objective of our quantitative meta-analysis is to identify general trends, key and strong points, to be considered in future studies of automatic depression assessment, given that a direct comparison of results is not viable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Categorical Depression Assessment</head><p>Approaches for categorical depression assessment presented in this section are grouped and compared in terms of the employed dataset, in accordance with <ref type="table" target="#tab_2">Table 3</ref>. Further, the results are considered with regard to the evaluated features, in reference to the taxonomy presented in <ref type="figure" target="#fig_3">Fig. 3</ref>. The various approaches, apart from reporting different performance metrics, were tested on datasets or particular subsets of varying sizes. Performance metrics in each report are explained next. Accuracy, which was reported in the majority of studies, is computed according to Equation (4) based on the following confusion matrix:</p><formula xml:id="formula_2">C Â¼ TP FN FP TN ;<label>(3)</label></formula><p>where TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives. Certain studies report "depressed accuracy", which implies sensitivity (or recall) given by <ref type="bibr" target="#b6">(6)</ref> Accuracy  <ref type="bibr" target="#b105">[107]</ref>, <ref type="bibr" target="#b120">[122]</ref>, <ref type="bibr" target="#b125">[127]</ref>, <ref type="bibr" target="#b126">[128]</ref>, <ref type="bibr" target="#b127">[129]</ref>, <ref type="bibr" target="#b146">[146]</ref>, <ref type="bibr" target="#b147">[147]</ref>, <ref type="bibr" target="#b169">[167]</ref> Linear Regression (LR) <ref type="bibr" target="#b90">[93]</ref>, <ref type="bibr" target="#b122">[124]</ref>, <ref type="bibr" target="#b146">[146]</ref> Partial Least Square Regression (PLS) <ref type="bibr" target="#b119">[121]</ref>, <ref type="bibr" target="#b122">[124]</ref> Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b142">[142]</ref>, <ref type="bibr" target="#b145">[145]</ref> Gaussian Staircase Regression (GSR) <ref type="bibr" target="#b169">[167]</ref>, <ref type="bibr" target="#b170">[168]</ref> Discriminative Mapping (DM) <ref type="bibr" target="#b147">[147]</ref> Moore-Penrose Generalized Inverse (MPGI) <ref type="bibr" target="#b142">[142]</ref> Relevance Vector Machines (RVM) <ref type="bibr" target="#b127">[129]</ref>, <ref type="bibr" target="#b170">[168]</ref> Extreme Learning Machines (ELM) <ref type="bibr" target="#b90">[93]</ref> Random Forest Regressor (RFR) <ref type="bibr" target="#b93">[96]</ref> Deep Convolutional Neural Networks (DCNN) <ref type="bibr" target="#b146">[146]</ref>  Where precision is given by</p><formula xml:id="formula_3">Â¼ TP Ã¾ TN TP Ã¾ TN Ã¾ FP Ã¾ FN :<label>(4)</label></formula><formula xml:id="formula_4">precision Â¼ TP TP Ã¾ FP ;<label>(5)</label></formula><p>and recall by</p><formula xml:id="formula_5">recall Â¼ TP TP Ã¾ FN :<label>(6)</label></formula><p>The F1 score, also reported in several studies, is given by</p><formula xml:id="formula_6">F 1 Â¼ 2 Ã precision Ã recall precision Ã¾ recall :<label>(7)</label></formula><p>The aforementioned performance metrics, however, fail to take chance agreement into consideration, which varies across different studies. To address this limitation and to </p><p>where p 0 is the proportion of accurately predicted decisions given by the accuracy formula as defined in <ref type="bibr" target="#b4">(4)</ref>, and p e the proportion of expected chance agreement, given by</p><formula xml:id="formula_8">p e Â¼ M a Ã¾ M b TP Ã¾ FN Ã¾ FP Ã¾ TN ;<label>(9)</label></formula><p>where M a and M b are defined as follows:</p><formula xml:id="formula_9">M a Â¼ Ã°TP Ã¾ FNÃž Ãƒ Ã°TP Ã¾ FP Ãž TP Ã¾ FN Ã¾ FP Ã¾ TN (10) M b Â¼ Ã°TN Ã¾ FP Ãž Ãƒ Ã°TN Ã¾ FNÃž TP Ã¾ FN Ã¾ FP Ã¾ TN :<label>(11)</label></formula><p>Whenever confusion matrices for user/gender independent depression assessment (based on one or more visual cues) were not included in the original publication, they were requested from the study authors. For the cases that the requested information was not provided, and if at least two performance metrics where reported in the original publication, along with the total number of subjects per class (depressed, non-depressed as defined in <ref type="bibr" target="#b12">(12)</ref> and (13)), a 4 Ã‚ 4 linear system of equations was solved in order to derive the confusion matrix</p><formula xml:id="formula_10">#depressed Â¼ TP Ã¾ FN (12) #not-depressed Â¼ TN Ã¾ FP:<label>(13)</label></formula><p>In the case of [98] a quadratic system was solved, since the reported metrics were averaged for the two classes (depressed/not-depressed). Finally, the computed confusion matrices were cross-checked to reproduce the originally reported performance metrics. If the estimated confusion matrices for a given study could not be verified by the reported performance metrics, the relevant study was not considered any further. It should be noted that Dibeklio glu et al. <ref type="bibr" target="#b166">[164]</ref> is the only reviewed publication which originally included Kappa statistics in their published report. <ref type="table" target="#tab_6">Table 7</ref> groups the reviewed studies according to the dataset used, the corresponding sample size and male rate. The table also lists the classification algorithm(s) used in each study, the features employed in the decision process, and the corresponding performance metric. Grouping of studies was inspired by <ref type="bibr" target="#b133">[134]</ref>. Studies are ranked by decreasing k value within each dataset-specific group. <ref type="table" target="#tab_7">Table 8</ref> presents similar information on studies using datasets or dataset combinations reported in single studies, precluding direct across-study comparisons. Finally, <ref type="figure" target="#fig_8">Fig. 8</ref> displays a forest plot of k values with the corresponding 95 percent confidence intervals given by</p><formula xml:id="formula_11">95%CI Â¼ k AE 1:96 Ãƒ s k ; (14) with s k defined as s k Â¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi p 0 Ã°1 Ã€ p 0 Ãž NÃ°1 Ã€ p e Ãž 2 s ;<label>(15)</label></formula><p>where N is the total number of samples. Below we attempt an assessment of the evidence provided by this metaanalysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Pittsburgh</head><p>The first report involving the Pittsburgh dataset is that of Cohn et al. <ref type="bibr" target="#b79">[82]</ref>, who presented results of three experiments on depression detection based on: a) manual annotation of AUs using AU14 (lip corner tightening), b) automatic detection with AAMs, and c) vocal prosody. They reported 88 percent accuracy in experiment (a), and 79 percent in both (b) and (c), stressing the need to adapt AAMs to individual patients <ref type="bibr" target="#b85">[88]</ref>. The authors conclude that non-verbal, vocal cues constitute a valid indicator of depression severity <ref type="bibr" target="#b89">[92]</ref>. Girard et al. <ref type="bibr" target="#b87">[90]</ref> extended this work by investigating the correlation of changes in patient clinical status with corresponding changes in facial expression and head motion patterns over four sessions at six-week intervals. They concluded that as depression severity was reduced, smiling became more frequent and expressions of contempt and embarrassment became less frequent <ref type="bibr" target="#b88">[91]</ref>.</p><p>The cross-cultural study of Alghowinem et al. <ref type="bibr" target="#b104">[106]</ref> was also tested on the Pittsburgh dataset among others, reporting an average recall of 94.7 percent. Dibeklio glu et al. <ref type="bibr" target="#b135">[136]</ref> tested several feature settings on the Pittsburgh dataset. More recently Dibeklio glu et al. <ref type="bibr" target="#b166">[164]</ref> presented a deep learning approach for detecting three levels of depression. Finally, Joshi et al. <ref type="bibr" target="#b111">[113]</ref>, and Joshi <ref type="bibr" target="#b108">[110]</ref>, reported 97.2 percent accuracy for assessing depression severity based on the Pittsburgh data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">BlackDog</head><p>McIntyre et al. <ref type="bibr" target="#b80">[83]</ref> were the first to report on the BlackDog dataset. Their results support the long-standing clinical observation that depressed individuals demonstrate lesser facial activity and a smaller repertoire of facial expressions. They also reported identifying two clusters of patients, those who showed psychomotor agitation and those who showed motor slowing <ref type="bibr" target="#b84">[87]</ref>, <ref type="bibr" target="#b86">[89]</ref>.</p><p>In a subsequent study, Joshi et al. <ref type="bibr" target="#b110">[112]</ref> focused on algorithm development and improvement and compared the performance of different neural network classification algorithms achieving depression detection accuracy as high as 88.3 percent. Higher performance, up to 91.7 percent, was achieved when additional modalities were included (speech, independent and relative movement of body parts) in Joshi et al. <ref type="bibr" target="#b109">[111]</ref> and Joshi et al. <ref type="bibr" target="#b121">[123]</ref>. Alghowinem et al. studied depression detection based on the analysis of either eye movements in <ref type="bibr" target="#b103">[105]</ref>, or head pose and head movements in <ref type="bibr" target="#b106">[108]</ref> by extracting 126 and 100 features, respectively. The maximum reported recall rate was 80 percent for the eye-based approach, and 82.6 percent for the head-based approach among women (corresponding rates were lower for men: 77 and 75.6 percent, respectively). In the same cross-cultural study mentioned in Section 5.1.1, Alghowinem et al. <ref type="bibr" target="#b104">[106]</ref> combined the two approaches (eyebased and head-based) achieving an average recall of 85 percent for a variable set of features (made specific for the BlackDog), and 76.7 percent for a fixed set of features along the different datasets employed in their cross-cultural study. Recently, Alghowinem et al. <ref type="bibr" target="#b114">[116]</ref> presented an improved performance of recall 88.3 percent, by extending their approach in terms of feature extraction, as well as machine learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">DAIC-WOZ</head><p>The creation of the DAIC-WOZ dataset was based on the SimSensei kiosk tool, developed at the Institute of Creative Technologies, University of Southern California (ICT-USC) <ref type="bibr" target="#b98">[100]</ref>, <ref type="bibr" target="#b100">[102]</ref>, <ref type="bibr" target="#b173">[170]</ref>, <ref type="bibr" target="#b174">[171]</ref>, <ref type="bibr" target="#b175">[172]</ref>. SimSensei is a virtual human dialogue system created to conduct personal interviews while recording and analyzing the facial image and speech of the interviewee. Several approaches have been tested on this dataset, mainly from the relevant research group, but also in AVEC'16, in terms of the depression sub-challenge.</p><p>Scherer et al. <ref type="bibr" target="#b99">[101]</ref> examined the value of the audiovisual approach, achieving 89.74 percent accuracy compared to 51.28 percent of single-modality acoustic features, and 64.10 percent for the single-modality visual features. In subsequent publications, Scherer et al. <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b101">[103]</ref>, examined linear associations between frequency of specific features and disorder type, showing significant differences between Post Traumatic Stress Disorder (PTSD), anxiety, depression, and distress on gaze behavior, smiling, self-touching and fidgeting.</p><p>Stratou et al. <ref type="bibr" target="#b81">[84]</ref> corroborated Alghowinem's findings <ref type="bibr" target="#b103">[105]</ref>, <ref type="bibr" target="#b106">[108]</ref> on gender differences in classification accuracy, reporting F1 scores of 0.858 for women and 0.808 for men in detecting presence of depression and PTSD. Yu et al. <ref type="bibr" target="#b97">[99]</ref> surmised that extracting and integrating features over the entire interview is not as accurate as extracting features from separate question-answer instances ("adjacencypairs"). Results for the adjacency-pairs approach revealed improved performance as indicated by an F1 score of 0.603 compared to a score of 0.523 for the data integrated over the entire session. The MaxEnt algorithm was used in both approaches.</p><p>Ghosh et al. <ref type="bibr" target="#b82">[85]</ref> integrated features derived from facial images and speech with affect (sadness, anxiety, anger) displayed in a chat-text and achieved improved classification accuracy of 66.40 percent as compared to 63.02 percent for unimodal text, 58.63 percent for speech, 58.00 percent for facial image, and 60.50 percent for the combination of facial image and speech.</p><p>Finally, DAIC-WOZ served as the benchmark dataset in terms of AVEC'16. Raw data were provided for audio recordings and transcripts, while for videos only features extracted with OpenFace were provided. Despite this limitation several interesting approaches were presented, with Yang et al. <ref type="bibr" target="#b146">[146]</ref> winning the depression sub-challenge by combining visual with audio and text features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">AVEC</head><p>The AVEC dataset, although intended for continuous assessment of depression, was also used for categorical assessment using case groupings based on BDI scores. For instance, Senoussaoui et al. <ref type="bibr" target="#b127">[129]</ref> achieved classification accuracy of 82 percent for categorical assessment of depression by using a cutoff score of 13/14 points on BDI, using the data organization provided by AVEC (training / development subsets).</p><p>Alghowinem et al. <ref type="bibr" target="#b104">[106]</ref>, in their cross-cultural study tested their algorithm on a subset of the AVEC data, in an effort to match the three datasets employed (Pittsburgh, BlackDog, and AVEC), both in terms of number of recordings as well as total duration. They reported an average recall of 68.8 percent for the fixed set of features across the three datasets.</p><p>Pampouchidou et al. <ref type="bibr" target="#b144">[144]</ref> considered standard BDI cutoffs: 0-9 (minimal depression), 10-18 (mild depression), 19-29 (moderate depression), and 30-63 points (severe depression). They reported 74.5 percent accuracy in discriminating cases of {minimal} versus {mild, moderate, severe}, and {minimal, mild, moderate} versus {severe} depressive symptomatology. The classification accuracy between {minimal, mild} versus {moderate, severe} was 63.5 percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Other Datasets</head><p>Datasets or data set combinations used in a single study are summarized in <ref type="table" target="#tab_7">Table 8</ref>. Alghowinem et al. <ref type="bibr" target="#b104">[106]</ref> attempted to merge several datasets (e.g., Pittsburgh and AVEC). This resulted in an improvement of classification performance as compared to relying solely on the AVEC dataset, reporting an average recall of 85.7 percent.</p><p>In one of the earliest studies, the corpus constructed by the Oregon Research Institute (ORI), was used to test the approach of Maddage et al. <ref type="bibr" target="#b137">[138]</ref> for video-based depression detection in adolescents. The corpus comprised recordings from eight adolescents (four in each class). They implemented both gender-dependent and independent classification achieving 75.6 percent recognition rate of adolescents with depression in the gender-independent case.</p><p>The ORYGEN dataset was employed for a more challenging endeavor undertaken by Ooi et al. <ref type="bibr" target="#b115">[117]</ref>. Facial expression analysis was utilized in order to predict whether initially non-depressed adolescents would develop depression at the end of a two-year follow-up period. They obtained baseline (Time 1) and two-year follow-up (Time 2) data from 191 non-depressed adolescents. At Time 2, 15 participants had developed depression. Given the still-disputed capacity of available computer based approaches to detect the current presence of depression, long-term prediction of depression onset was a rather optimistic goal. As expected, results revealed relatively poor prediction accuracy (50 percent for person-independent, and 61 percent for person-dependent) casting doubt on the sensitivity of facial features as prodromal signs of clinical depression <ref type="bibr" target="#b123">[125]</ref>.</p><p>Zhou et al. <ref type="bibr" target="#b72">[75]</ref> at the University of Rochester departed from the traditional laboratory settings and obtained data in realistic conditions. Participants interacted with each other through social networking, while sitting in an area customized to resemble a living room. Pupil radius, head movement rate, eye blinking rate, text-derived affect, and keystroke rate are some of the cues they considered for detecting the presence of depressive symptomatology. They reported 0.817 precision and 0.739 recall for classifying patients versus healthy volunteers reporting high levels of negative mood. Although promising, the generalizability of these results is questionable given the small number of patients with depression (n = 5).</p><p>Finally, recent results from the CHI-MEI dataset <ref type="bibr" target="#b92">[95]</ref>, attempting to distinguish unipolar depression (MDD) from bipolar disorder, reached 65.38 percent classification accuracy when combining AUs and audio features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Continuous Depression Assessment</head><p>The majority of the approaches reviewed here are based on AVEC datasets as participations to the actual challenge, or as independently published studies. The depression challenge of AVEC 2013 and AVEC 2014 required prediction of individual BDI scores based on corresponding video recordings (both visual and speech data are available). Video recordings were divided into three subsets (training, development and testing), with labels being released only for the first two. AVEC 2013 included the complete recordings of the participants executing 12 tasks, while for AVEC 2014 only two tasks were kept: the Northwind (reading a novel excerpt) and Freeform (answering to an open question). AVEC 2016, although focused on categorical assessment, encouraged participants to address prediction of self-reported scores on the PHQ-8 scale, employed by DAIC-WOZ.</p><p>In all cases, performance metrics were the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) given by Equations <ref type="bibr" target="#b16">(16)</ref> and <ref type="bibr" target="#b17">(17)</ref>, where n is the number of samples, p the predicted value, and a the actual value</p><formula xml:id="formula_12">RMSE Â¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 n X n iÂ¼1 Ã°p i Ã€ a i Ãž 2 s (16) MAE Â¼ 1 n X n iÂ¼1 p i Ã€ a i j j:<label>(17)</label></formula><p>The MIT-Lincoln team won both 2013 and 2014 challenges <ref type="bibr" target="#b90">[93]</ref>, <ref type="bibr" target="#b176">[173]</ref>. In their first participation they achieved subject-independent RMSE/MAE of 8.5/6.53 on the test set, 8.68/7.12 on the development, and 7.42/5.75 subject-dependent adaptation on the development set, relying solely on vocal features. By incorporating AU-based features, the team achieved slightly better results in the subsequent challenge on a different subset (8.12/6.31 on the test set; results for the development set were not reported). In a recent article, Zhu et al. <ref type="bibr" target="#b149">[149]</ref>, presented a deep learning approach tested on the datasets provided by AVEC'13 and AVEC'14.</p><p>Their method was single-visual and deep learning based, outperforming other reported single-visual approaches, and comparing favorably with the overall best performing (multimodal) results (cf. <ref type="table" target="#tab_8">Table 9)</ref>, with RMSE/MAE of 9.55/7.47 for AVEC'14, and 9.82/7.58 for AVEC'13 respectively. In terms of AVEC 2016, Williamson et al. <ref type="bibr" target="#b169">[167]</ref> and Yang et al. <ref type="bibr" target="#b146">[146]</ref> addressed prediction of PHQ-8, with the latter predicting scores separately for females and males. <ref type="table" target="#tab_8">Table 9</ref> compares the performance of each algorithm when tested on visual versus multimodal signals. The regression algorithm, along with the fusion strategy and the respective scores, for both development and test subsets, are also presented where available. Upward arrows indicate better performance of the multimodal versus the visual approach alone and the opposite is indicated by downward arrows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Concluding comments are organized according to the datasets and algorithms used in each of the reviewed studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Algorithm and Performance Related Issues</head><p>Given the temporal variability of depressive manifestations, the majority of facial signs utilized in the reviewed studies are dynamic, while the occurrence of static signs is typically considered over time (e.g., smile frequency). Therefore, video recordings, as opposed to static images, are required. Further, a trend toward utilizing high-level features has been observed; this trend was also promoted by AVEC <ref type="bibr">'16,</ref> in which features were provided after preprocessing, that enabled high-level feature extraction. Furthermore, the majority of top performing methods employ multiple sets of features, across or even within a given modality, e.g., visual cues from face and body.</p><p>With respect to the continuous assessment approaches, multimodal methods (audio/visual) clearly outperform those relying on solely on visual cues. Thus, as indicated by the arrows in <ref type="table" target="#tab_8">Table 9</ref>, the inclusion of audio cues resulted in performance improvement in 17 out of 22 methods reporting on both single-visual and multimodal performance. Furthermore, decision fusion appears to be the most prominent. Potential benefits from multimodal approaches have also been discussed in <ref type="bibr" target="#b61">[64]</ref>. Finally, gender-based classification has been supported by many of the reported approaches to perform better <ref type="bibr" target="#b83">[86]</ref>, <ref type="bibr" target="#b114">[116]</ref>, <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b146">[146]</ref>, <ref type="bibr" target="#b150">[150]</ref>.</p><p>Deep learning based approaches have been found in only two recent articles. In the first, Dibeklio glu et al. <ref type="bibr" target="#b166">[164]</ref> attempted a multimodal approach, employing Stacked Denoising Autoencoders for detecting three levels of depression, and reported comparable results to two-level approaches. Also, Zhu et al. <ref type="bibr" target="#b149">[149]</ref> addressed the AVEC'13 and AVEC'14 depression sub-challenges using Deep Convolutional Neural Networks, achieving the highest performance among the single-visual approaches, although underperforming the multimodal ones. Deep learning seems to be promising, demanding further exploration. It is worth noting that multimodal approaches are again performing best.</p><p>Treating depression as a continuous variable (i.e., reflecting depression severity) is gaining ground over categorical decision systems (e.g., as reflected in AVEC'13 and AVEC <ref type="bibr">'14)</ref>. This is due to the fact that, despite the apparent simplicity of categorical assessment, it does not represent neither properly nor reliably the complex nature of mood disorders. Efforts to develop methods capable of differentiating mood disorder types, and also depression from other psychiatric disorders, such as various anxiety conditions, are limited. However, there have been some notable initial attempts in this direction by ICT-USC focusing on distinguishing PTSD from depression <ref type="bibr" target="#b81">[84]</ref>, <ref type="bibr" target="#b83">[86]</ref>, and by CHI-MEI attempting to distinguish cases of unipolar depression versus bipolar disorder <ref type="bibr" target="#b92">[95]</ref>. It is worth pointing out that, regarding long-term prediction of depression onset, available results reveal relatively poor prediction accuracy calling into question the significance of facial manifestations as prodromal signs of depression.</p><p>Finally, although reported detection accuracy rates can be very high, a fact that clearly demonstrates the clinical potential of the field, sample sizes are often too small to enable the generalizability of these results. In order for a system to be fully evaluated and acknowledged as an assessment tool, it must be tested on considerably larger sample sizes, featuring a wider variety of demographic characteristics, clinical diagnosis methods, and ethniccultural backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data Related Issues</head><p>An initial observation based on the meta-analysis performed in Section 5.1, summarized by Tables 7 and 8, and <ref type="figure" target="#fig_8">Fig. 8</ref>, permits a rough comparison of methods, through Cohen's k (a chance-robust metric). Approaches tested on the Pittsburgh dataset achieved higher (in their majority above average) performance. Similar results were reported by Alghowinem et al. <ref type="bibr" target="#b104">[106]</ref>, who tested the exact same algorithm on different datasets (cf. Tables 7 and 8, and <ref type="figure" target="#fig_8">Fig. 8</ref>). This finding highlights the importance of data quality (sample size, noise, resolution, environment, etc.) for the development and testing of a given computational pipeline. Further, all participants in the Pittsburgh dataset met DSM criteria for clinical depression, ensuring diagnostic uniformity and exclusion of potentially important comorbid conditions <ref type="bibr" target="#b166">[164]</ref>. Finally, the data were obtained during a clinical interview, thus enhancing the interpersonal context <ref type="bibr" target="#b166">[164]</ref>.</p><p>Regarding the categorical approaches tested on the AVEC and DAIC-WOZ datasets, there are some additional potential reasons which could have affected the performance, as revealed through comparison of k values cross studies. First, inclusion of participants from different ethnicities than other datasets may have biased the result of such experimental manipulations as mood induction. Further, the AVEC and DAIC-WOZ datasets were collected in a way that limited the audience effect (human-computer interaction setup), eliminating cues that could only occur in a social context as, for instance, indicated by Dibeklio glu et al. <ref type="bibr" target="#b166">[164]</ref>. Also, the Pittsburgh and BlackDog datasets were both based on clinical diagnosis, as opposed to AVEC and DAIC-WOZ, which were based on self-reports; a factor expected to affect annotation accuracy <ref type="bibr" target="#b114">[116]</ref>.</p><p>Most importantly, the problem addressed with the AVEC and DAIC-WOZ datasets, which relied on self-reported symptoms for data annotation, is far more complex in comparison to the other datasets. While others focused on categorical assessment (healthy versus depressed, high versus low depression severity) based on clinical diagnosis, AVEC focused on the prediction of individual BDI scores. Even during AVEC'16, where DAIC-WOZ was used for categorical assessment, binary labels (depressed/not-depressed) were again based on participant symptom self-report (PHQ-8 scores). As explained in the Clinical Background section, Self-RI scores depend on a variety of biasing factors (subjective, social, etc.). Thus, although depression can be accurately portrayed by facial expressions, the ground-truth may not accurately measure depression severity in these studies. This could also justify the fact that reported approaches for categorical assessment outnumber the ones for continuous assessment, as the continuous prediction is more challenging.</p><p>So what constitutes an optimal data set? It appears that it should be comprised of a number of patients diagnosed by experienced psychiatrists, using largely uniform diagnostic criteria. Comorbid diagnoses, should be carefully recorded and later used to evaluate potential misclassifications, since significant correlations have been observed between PTSD, anxiety and depression <ref type="bibr" target="#b101">[103]</ref>. Consequently, the development of an algorithmic tool for depression assessment, apart from the contribution of engineers and computer scientists, would most definitely require direct supervision by clinicians. Furthermore, as reported in the clinical background of Section 1.1, a one-off assessment may not be sufficient, as development of rapport with the participant is necessary. For this to be achieved several sessions over a fixed interval (e.g., 7 weeks as in <ref type="bibr" target="#b79">[82]</ref>, <ref type="bibr" target="#b87">[90]</ref>, <ref type="bibr" target="#b88">[91]</ref>) are advisable. Existence of baseline data would also be useful, but unfortunately this is not possible in most cases. However multiple sessions can benefit the remission assessment, allowing long-term monitoring of the recovery process.</p><p>With respect to video acquisition parameters, average image resolution and frame raterate are typically reported. However, it is not clear whether image acquisition with higher-level specifications could improve assessment accuracy. A quantitative comparison of different resolutions and frame rates employing the same algorithm(s) may be required to conclusively address this question. The size of the sample is also an important factor to be considered, as size of the majority of reported datasets is at best moderate to allow generalizations <ref type="bibr" target="#b114">[116]</ref>.</p><p>In the field of automatic facial expression recognition (AFER) approaches are moving toward real-world conditions <ref type="bibr" target="#b59">[62]</ref>, as exemplified by the Emotion Recognition inthe-Wild (EmotiW) challenge series <ref type="bibr" target="#b177">[174]</ref>, <ref type="bibr" target="#b178">[175]</ref>, <ref type="bibr" target="#b179">[176]</ref>. The manner in which the AVEC dataset was constructed also supports this idea, as the recordings took place in independent setups and on personal computers. This choice, however, impacted performance, as shown in <ref type="table" target="#tab_6">Table 7</ref> and <ref type="figure" target="#fig_8">Fig. 8</ref>: approaches for categorical assessment of depression based on AVEC demonstrated lower than average performance, when compared to approaches based on other datasets. Additionally, although current in-the-wild approaches may be considered as promising, they are not yet sufficiently reliable even for AFER as supported in <ref type="bibr" target="#b58">[61]</ref>, <ref type="bibr" target="#b59">[62]</ref>. Therefore, at present, such approaches do not appear to meet minimum requirements for a clinical decision support system. On the other hand, the strict requirement for standardization of data collection <ref type="bibr" target="#b61">[64]</ref> may impose potentially serious limitations, such as questionable originality and genuineness of the data and lack of variance in contextual information. Although standardized medical equipment typically operate under highly controlled conditions, collection of data indicative of depressive symptomatology is highly susceptible to the dynamic nature of behavioral and underlying psychological processes of the person being evaluated.</p><p>The most important issue concerning the data is availability. As mentioned before, obscuring participant identity is practically impossible in raw video data compared to other modalities (e.g., speech or physiological signals). Consequently, open access is strictly prohibited, while licensing to a third party is seriously restricted. A possible solution to this problem would be for the academic community to promote cooperation between institutions, as suggested by Cummins et al. <ref type="bibr" target="#b61">[64]</ref>, so that the data could be shared under regulated conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS &amp; FUTURE WORK</head><p>Research on automatic depression assessment has come a long way from Cohn et al. <ref type="bibr" target="#b79">[82]</ref> and McIntyre et al. <ref type="bibr" target="#b80">[83]</ref>, with several novel approaches both in terms of methodology and performance. The present comprehensive review of the state-of-the-art provides a number of insights, while identifying many questions open to further investigation. Depression diagnosis itself is an active and controversial topic in clinical psychology and psychiatry. Given the aforementioned outstanding issues, the development of automated, objective assessment methods may be valuable for both research and clinical practice.</p><p>In general, results are consistent with the social withdrawal <ref type="bibr" target="#b83">[86]</ref>, <ref type="bibr" target="#b87">[90]</ref>, emotion-context insensitivity <ref type="bibr" target="#b87">[90]</ref>, and reduced reactivity <ref type="bibr" target="#b166">[164]</ref> hypotheses of depression. Additionally, the importance of dynamic features, as well as multimodal approaches, was also highlighted through the quantitative analysis reported in this review. Continuous approaches appear to be more in accordance with clinical experience. Sharing of related data needs to be promoted, and the data collection procedures need to be standardized on several domains, in order to enhance reliability and comparability of results. In terms of related algorithms, although a multitude of approaches is reported in the literature, automatic depression assessment is still a long way from being well established, with significant room for improvement on current methods. Given the dynamic of deep learning, and its considerably high performance in many related fields, if large enough datasets are provided, an impact in automatic depression assessment is also expected in the near future.</p><p>Several clinical research questions remain to be addressed systematically, such as the capacity to distinguish between different depression subtypes, and MDD from other mood disorders <ref type="bibr" target="#b83">[86]</ref>. Individual variability due to comorbid personality disorders or characteristics, as well as the influence of ethnicity and culture requires further exploration <ref type="bibr" target="#b114">[116]</ref>. Interestingly, although physiological activity measured through EMG, BVP, skin conductance, and respiration can be informative regarding ongoing emotional responses <ref type="bibr" target="#b180">[177]</ref>, such information has not been integrated in the reviewed multimodal studies, with the exception of Zhou et al. <ref type="bibr" target="#b72">[75]</ref>, who recorded heart rate via a non-contact, facial video-based system. Accordingly, body manifestations, as well as pupil related features, have not been adequately employed for automatic assessment, although they have been proven statistically significant. Finally, investigation of facial signs in the context of dyadic interaction is a forthcoming domain for exploration <ref type="bibr" target="#b101">[103]</ref>, still only one approach considered it <ref type="bibr" target="#b97">[99]</ref>, while context has not been considered in any study.</p><p>In conclusion, the exhaustive review of available evidence highlights the considerable potential in the application of video-based methods for the assessment and monitoring of the course of depression. It was further made apparent that visual cues need to be supplemented by information from other modalities to achieve clinically useful results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Anastasia Pampouchidou was funded by the Greek State Scholarship Foundation, under the scholarship programme instituted in memory of Maria Zaousi. This work was also partially supported by a grant entitled SEMEOTICONS (SEMEiotic Oriented Technology for Individual's Cardi-Ometabolic risk self-assessmeNt and Self-monitoring) within the framework of the FP7 Specific Targeted Research Project partially funded by the European Commission under Grant Agreement 611516. The authors would further like to express their gratitude to Prof. Wu and to Drs. Alghowinem, Joshi, Dibeklio glu, and Senoussaoui for providing unpublished confusion matrices of their approaches. Finally, the authors express their gratitude to the anonymous reviewers of earlier versions of this work for their constructive comments and suggestions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Number of studies in the field of depression assessment by year of publication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Section 4.2.1 and illustrated in Fig. 3, while methods for dimensionality reduction and feature level fusion are reported in Sections 4.2.2 and 4.2.3, respectively. Machine learning algorithms are employed, depending on the research question, i.e., presence of depression or severity assessment. Classification approaches are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Workflow for automatic depression assessment. The output can be derived from a) single feature sets/modalities, b) feature fusion, with dimensionality reduction before (FF1) or after fusion (FF2), and c) decision fusion with any possible combination of outputs from single feature sets/ modalities and feature fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Taxonomy of visual features utilized in the reviewed studies for depression assessment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>CLM fitted on the face (created using algorithm from<ref type="bibr" target="#b162">[160]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>AU examples typical of depression: a. AU01 inner brow raiser (sadness), b. AU04 brow lowerer (sadness), c. AU11 nasolabial furrow deepener (distress), d. AU15 lip corner depressor (distress).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Ranking of classification algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Ranking of regression algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Forest plot of study-specific k values, grouped by dataset. The vertical dashed line corresponds to the average k Â¼ 0:54.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Search</head><label>1</label><figDesc></figDesc><table><row><cell>Terms and Web-Resources Employed</cell></row><row><cell>in the Current Review</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Non</head><label>2</label><figDesc></figDesc><table><row><cell>-Verbal Manifestations of Depression</cell></row><row><cell>Pupil dilation/bias</cell></row><row><cell>Pupillary response</cell></row><row><cell>Iris movement</cell></row><row><cell>Eyelid activity (openings, blinking)</cell></row><row><cell>Saccadic eye movements</cell></row><row><cell>Eye gaze (limited &amp; shorter eye contact)</cell></row><row><cell>Visual fixation</cell></row><row><cell>Low frequency &amp; duration of glances Extended activity on the corrugator 1 muscle</cell></row><row><cell>Eyebrow activity "Veraguth fold" 2</cell></row><row><cell>Frowns</cell></row><row><cell>Fewer smiles</cell></row><row><cell>More frequent lip presses</cell></row><row><cell>Smile intensity &amp; duration</cell></row><row><cell>Mouth corners angled down</cell></row><row><cell>Mouth animation</cell></row><row><cell>Listening smiles (smiling while not speaking) Reduced activity on the zygomaticus 3</cell></row><row><cell>Facial activity</cell></row><row><cell>Action Units occurrence (mean duration, ratio of fonset/</cell></row><row><cell>total duration, onset/offsetg)</cell></row><row><cell>Region Units (RUs)</cell></row><row><cell>Facial expression occurrence (variability &amp; intensity)</cell></row><row><cell>Sad/negative/neutral expression occurrence</cell></row><row><cell>Head pose (orientation, movement)</cell></row><row><cell>Body gestures (full or upper body, or body parts)</cell></row><row><cell>Slumped posture</cell></row><row><cell>Limp &amp; uniform body posture</cell></row><row><cell>Reduced &amp; slowed arm and hand movements</cell></row><row><cell>Shaking and/or fidgeting behavior</cell></row><row><cell>Self-adaptors</cell></row><row><cell>Foot tapping</cell></row><row><cell>Motor variability</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 Datasets</head><label>3</label><figDesc>Employed by the Reviewed Studies for Depression Assessment</figDesc><table><row><cell>Corpus</cell><cell>Population /</cell><cell>Symptomatology</cell><cell cols="2">Ground Truth Selection Criteria</cell><cell>Research</cell><cell>Image Resolution /</cell><cell>Availability to Third Parties</cell></row><row><cell></cell><cell>Total (Control /</cell><cell>Collection Methods</cell><cell></cell><cell></cell><cell>Question</cell><cell>Frame Rate</cell><cell></cell></row><row><cell></cell><cell>Study)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pittsburgh</cell><cell cols="2">Adults / 49 (-/-) Interpersonal</cell><cell>Clinical</cell><cell>DSM-IV, HAM-</cell><cell>Detection</cell><cell>640x480 / 29.97</cell><cell>Visual &amp; Audio Features</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Assessment</cell><cell>D &gt; 15</cell><cell></cell><cell></cell><cell>(Expected)</cell></row><row><cell>BlackDog</cell><cell>Adults /</cell><cell>Combination</cell><cell>Clinical</cell><cell>Control: No</cell><cell>Detection</cell><cell>800x600 / 24.94</cell><cell>-</cell></row><row><cell></cell><cell>130 (70/60)</cell><cell></cell><cell>Assessment</cell><cell>history of</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>mental illness,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Study:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DSM-IV, HAM-</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>D &gt; 15</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DAIC-WOZ Adults /</cell><cell>Combination</cell><cell>Self-report</cell><cell>Age, language,</cell><cell>Detection,</cell><cell>-</cell><cell>Visual &amp; Audio Features,</cell></row><row><cell></cell><cell>189 (-/-)</cell><cell></cell><cell></cell><cell>eye-sight</cell><cell>Severity</cell><cell></cell><cell>Audio Recordings,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Transcripts</cell></row><row><cell>AVEC</cell><cell>Adults 58</cell><cell>Non-social</cell><cell>Self-report</cell><cell>-</cell><cell>Severity</cell><cell>-</cell><cell>Full Video Recordings,</cell></row><row><cell></cell><cell>(-/-)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Visual &amp; Audio Features</cell></row><row><cell>ORI</cell><cell>Adolescents /</cell><cell>Interpersonal</cell><cell>-</cell><cell>-</cell><cell>Detection</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>8 (4/4)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ORYGEN</cell><cell>Adolescents /</cell><cell>Interpersonal</cell><cell>Clinical</cell><cell>Stage1: No</cell><cell>Prediction</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>30 (15/15)</cell><cell></cell><cell>Assessment</cell><cell>depression,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>age 9-12 years</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stage2:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Depression, 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>years after</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CHI-MEI</cell><cell>Adults /</cell><cell>Combination</cell><cell>Clinical</cell><cell>DSSS, HAM-D</cell><cell>Unipolar</cell><cell>640x480 / 30</cell><cell>-</cell></row><row><cell></cell><cell>26 (13/13)</cell><cell></cell><cell>Assessment</cell><cell></cell><cell>Depression /</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bipolar</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Disorder</cell><cell></cell><cell></cell></row><row><cell>EMORY</cell><cell>Adults /</cell><cell>Interpersonal</cell><cell>Clinical</cell><cell>DBS-SCC Treat-</cell><cell>Recovery</cell><cell>-/30</cell><cell>-</cell></row><row><cell></cell><cell>7 (-/7)</cell><cell></cell><cell>Assessment</cell><cell>ment</cell><cell></cell><cell></cell><cell></cell></row></table><note>been used to investigate clinical severity of depression based on eye temperature</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 Features</head><label>4</label><figDesc>Associated with Statistically Significant Differences Between Study and Control Groups, as Reported in the Original Papers (Similar to Cummins et al.<ref type="bibr" target="#b61">[64]</ref>)</figDesc><table><row><cell>Feature</cell><cell>Reference</cell><cell>Study Group</cell><cell>Population</cell><cell>Control</cell><cell>Study</cell><cell>Significance</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(Control/Study) /</cell><cell>(mean AE S.D.)</cell><cell>(mean AE S.D.)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Male rate</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Saccade latency</cell><cell>Winograd-Gurvich</cell><cell>Melancholic</cell><cell>24 (15/9)/25%</cell><cell>151.3 AE 16.7</cell><cell>172.0 AE 22.1</cell><cell>p &lt; 0.05 (post hoc)</cell></row><row><cell></cell><cell>et al. (2006) [80]</cell><cell>depression</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Variability of saccade</cell><cell>Winograd-Gurvich</cell><cell>Melancholic</cell><cell>24 (15/9)/25%</cell><cell>24.4 AE 77.0</cell><cell>48.1 AE 35.7</cell><cell>p &lt; 0.05 (post hoc)</cell></row><row><cell>latency</cell><cell>et al. (2006) [80]</cell><cell>depression</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average base rate</cell><cell>Girard et al. (2013)</cell><cell>High vs low</cell><cell>19 (-/-)/36.8%</cell><cell>17.0%</cell><cell>27.8%</cell><cell>p &lt; 0.05 (Wilcoxon</cell></row><row><cell>(ABR) of AU14</cell><cell>[90]</cell><cell>depression severity</cell><cell></cell><cell></cell><cell></cell><cell>signed rank test)</cell></row><row><cell>ABR of AU15</cell><cell>Girard et al. (2013)</cell><cell>High vs low</cell><cell>19 (-/-)/36.8%</cell><cell>16.5%</cell><cell>8.5%</cell><cell>p &lt; 0.05 (Wilcoxon</cell></row><row><cell></cell><cell>[90]</cell><cell>depression severity</cell><cell></cell><cell></cell><cell></cell><cell>Signed Rank test)</cell></row><row><cell>Average mean square</cell><cell>Girard et al. (2013)</cell><cell>High vs low</cell><cell>19 (-/-)/36.8%</cell><cell>0.0029</cell><cell>0.0013</cell><cell>p &lt; 0.05 (Wilcoxon</cell></row><row><cell>of head motion</cell><cell>[90]</cell><cell>depression severity</cell><cell></cell><cell></cell><cell></cell><cell>signed rank test)</cell></row><row><cell>(AMSHM) vertical</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>amplitude</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AMSHM vertical</cell><cell>Girard et al. (2013)</cell><cell>High vs low</cell><cell>19 (-/-)/36.8%</cell><cell>0.0005</cell><cell>0.0001</cell><cell>p &lt; 0.01 (Wilcoxon</cell></row><row><cell>velocity</cell><cell>[90]</cell><cell>depression severity</cell><cell></cell><cell></cell><cell></cell><cell>signed rank test)</cell></row><row><cell>AMSHM horizontal</cell><cell>Girard et al. (2013)</cell><cell>High vs low</cell><cell>19 (-/-)/36.8%</cell><cell>0.0034</cell><cell>0.0014</cell><cell>p &lt; 0.01 (Wilcoxon</cell></row><row><cell>amplitude</cell><cell>[90]</cell><cell>depression severity</cell><cell></cell><cell></cell><cell></cell><cell>Signed Rank test)</cell></row><row><cell>AMSHM horizontal</cell><cell>Girard et al. (2013)</cell><cell>High vs low</cell><cell>19 (-/-)/36.8%</cell><cell>0.0005</cell><cell>0.0002</cell><cell>p &lt; 0.01 (Wilcoxon</cell></row><row><cell>velocity</cell><cell>[90]</cell><cell>depression severity</cell><cell></cell><cell></cell><cell></cell><cell>signed rank test)</cell></row><row><cell>Pupil area in</cell><cell cols="2">Wang et al. (2014) [74] Depression</cell><cell>30 (15/15)/0%</cell><cell>26.0 AE 5.1</cell><cell>32.4 AE 5.4</cell><cell>p &lt; 0.01 (t-test)</cell></row><row><cell>darkness</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pupil area in largest</cell><cell cols="2">Wang et al. (2014) [74] Depression</cell><cell>30 (15/15)/0%</cell><cell>16.9 AE 3.8</cell><cell>20.8 AE 5.8</cell><cell>p &lt; 0.05 (t-test)</cell></row><row><cell>constriction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Smile intensity</cell><cell>Scherer et al. (2014)</cell><cell>Depression</cell><cell>111 (79/32)/-</cell><cell>19.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>[98]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>9 AE 16.9 12.8 AE 11.1 p &lt; 0.05 (t-test)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 Classification</head><label>5</label><figDesc>Algorithms Employed in the Reviewed Studies</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Regression Algorithms Employed in the Reviewed StudiesSVR</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>Comparison of Approaches for Categorical Assessment of Depression Grouped According to the Dataset Used, Ranked within Group Based on Kappa</figDesc><table><row><cell cols="3">permit direct comparisons between classification</cell></row><row><cell cols="3">approaches, Cohen's Kappa statistic [169], a chance and</cell></row><row><cell cols="3">skew robust metric, was computed whenever possible. It is</cell></row><row><cell cols="3">based on the confusion matrix (3), and given by</cell></row><row><cell>k Â¼</cell><cell>p 0 Ã€ p e 1 Ã€ p e</cell><cell>;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc>Approaches for Categorical Assessment of Depression Employing Datasets, or Combination of Datasets, Which Have Not Been Reported Elsewhere Confusion matrix was computed, and not provided by the authors of the original research report.</figDesc><table><row><cell>Data</cell><cell>Paper</cell><cell>Population (Study/</cell><cell>Features</cell><cell>Classification</cell><cell>Reported Accuracy</cell><cell>Kappa</cell></row><row><cell></cell><cell></cell><cell>Control) / Male rate</cell><cell></cell><cell>Algorithm</cell><cell>(or precision)</cell><cell></cell></row><row><cell>Pittsburgh +</cell><cell>Alghowinem et al. (2015) [106]</cell><cell>70 (35/35) / 32.9%</cell><cell>Eyes, Full Face</cell><cell>SVM</cell><cell>mean recall = 85.7%</cell><cell>0.57</cell></row><row><cell>AVEC'14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rochester</cell><cell>Zhou et al. (2015) [75]</cell><cell>10 (5/5) /-</cell><cell>Full Face, Eyes</cell><cell>Logistic regression</cell><cell>precision = 0.82</cell><cell>0.57</cell></row><row><cell>ORI</cell><cell>Maddage et al. (2009) [138]</cell><cell>8 (4/4) / 50%</cell><cell>Full Face</cell><cell>GMM</cell><cell>75.6%</cell><cell>0.45*</cell></row><row><cell>ORYGEN</cell><cell>Ooi et al. (2011) [117], [125]</cell><cell>30 (15/15)/ 51%</cell><cell>Full Face</cell><cell>Nearest Neighbour</cell><cell>51%</cell><cell>0.03*</cell></row><row><cell>CHI-MEI</cell><cell>Yang et al. (2016) [95]</cell><cell>26 (13/13) / -</cell><cell>AUs, Audio</cell><cell>CHMM</cell><cell>65.38%</cell><cell>0.26</cell></row><row><cell></cell><cell>Yang et al. (2016) [95]</cell><cell>26 (13/13) / -</cell><cell>AUs</cell><cell>HMM</cell><cell>53.85%</cell><cell>0.08</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9</head><label>9</label><figDesc>Summary of Methods and Results of Studies Employing Continuous Depression Assessment Based on the Dataset Provided by AVEC'13, AVEC'14 and AVEC'16</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Corrugator is the muscle close to the eye, in the medial extremity of the eyebrow.2 "veraguth fold" is a fold (wrinkle) of skin on the upper eyelid, between the eyebrows.<ref type="bibr" target="#b3">3</ref> zygomaticus is the muscle which draws the angle of the mouth to produce a smile.PAMPOUCHIDOU ET AL.: AUTOMATIC ASSESSMENT OF DEPRESSION BASED ON VISUAL CUES: A SYSTEMATIC REVIEW</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">PAMPOUCHIDOU ET AL.: AUTOMATIC ASSESSMENT OF DEPRESSION BASED ON VISUAL CUES: A SYSTEMATIC REVIEW</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fan Yang received the BS degree in electrical engineering from the University of Lanzhou, China, in 1982 and the MS (computer science) and PhD degrees (image processing) from the University of Burgundy, France, in 1994 and 1998, respectively. She is currently a full professor and member of LE2I UMR CNRS, Laboratory of Electronic, Computing, and Imaging Sciences, University of Burgundy, France. Her research interests include the pattern recognition, neural network, parallelism and real time implementation, and, more specifically, automatic face image processing algorithms and architectures. She is member of the French research group ISIS (Information, Signal, Images and Vision), she livens up the theme C: Algorithm Architecture Mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matthew Pediaditis received the engineer's and PhD degrees in biomedical engineering from</head><p>Graz University of Technology, Austria. He has collaborated in numerous research projects in the domains of eHealth, ambient assisted living and patient health monitoring. He has expertise in video analysis for health applications using computer vision and machine learning. His current research focuses on human motion and facial expression analysis for disease assessment in epilepsy.</p><p>Manolis Tsiknakis received the BEng degree in electric and electronic engineering, in 1983, the MSc degree in microprocessor engineering, in 1985, and the PhD degree in systems engineering from the University of Bradford, Bradford, United Kingdom, in 1989. From February 1992 until January 2012 he has been with the Institute of Computer Science, Foundation for Research and Technology-Hellas, Greece, as a principal researcher and head of the center of eHealth Technologies. He is currently a professor of biomedical informatics in the Department of Informatics Engineering, Technological Educational Institute of Crete and a visiting researcher at FORTH/ICS. He led the effort for the design and implementation of HYGEIAnet, the regional health informatics network of Crete, an eEurope award winner in 2002 and was FORTH's best applied research award winner in 2003. He is currently an associate editor in the IEEE Journal of Biomedical and Health Informatics. He has published extensively -more than 250 papers -in refereed scientific journals and international conferences on issues related to the application of innovative ICT in the domain of clinical and translational research, care and wellness management. His current research interests include biomedical informatics and engineering, approaches for semantic health data integration, service oriented SW architectures and cloud computing and their application in biomedicine, affective computing, behavioral modeling and human activity recognition using wearable sensors, smart eHealth and mHealth service platforms.</p><p>" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Maximum Entropy Model</title>
		<imprint/>
	</monogr>
	<note>MaxEnt) [85], [99</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">World Health Organization</title>
		<ptr target="http://www.who.int/mental_health/management/depression/en/" />
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">American Psychiatric Association, Diagnostic and Statistical Manual of Mental Disorders (DSM-5 Ã’ )</title>
		<imprint>
			<date type="published" when="2013" />
			<publisher>American Psychiatric Publishing</publisher>
			<pubPlace>Arlington, VA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The costs of depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychiatric Clinics North America</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introduction: Depression, one and many</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Wakefield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Demazeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sadness or Depression? International Perspectives on the Depression Epidemic and Its Meaning</title>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Survey of Health, Ageing and Retirement in Europe</title>
		<ptr target="http://www.share-project.org" />
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A mobile system for treatment of depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tousset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Paradigms for Mental Health</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relationship of suicide rates to economic variables in Europe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Fountoulakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. J. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="486" to="496" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cost of depression in Europe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sobocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jâ‚¬ Onsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rehnberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mental Health Policy Economics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The economic cost of brain disorders in Europe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Neurology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The epidemiology of depression across cultures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Bromet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Public Health</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="119" to="138" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The current status of the diagnosis of depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sadness or Depression? International Perspectives on the Depression Epidemic and Its Meaning</title>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interaction of cognitive avoidance coping and stress in predicting depression/anxiety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J E</forename><surname>Joiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Therapy Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="65" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Structured Clinical Interview for DSM-IV Axis I Disorders (SCID-I) and the Structured Clinical Interview for DSM-IV Axis II Disorders (SCID-II)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>First</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comprehensive handbook of psychological assessment</title>
		<editor>. M. J. Hilsenroth and D. L. Segal</editor>
		<meeting><address><addrLine>Hoboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="134" to="143" />
		</imprint>
	</monogr>
	<note>Personality assessment</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A rating scale for depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurology Neurosurgery Psychiatry</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hamilton depression rating scale: Has the gold standard become a lead weight?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Bagby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. J. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2163" to="2177" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Method matters: Understanding diagnostic reliability in DSM-IV and DSM-5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chmielewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Bagby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Art. no. 764</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DSM-5: How reliable is reliable enough?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Kraemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kupfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Narrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Regier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. J. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="15" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diagnosing depression: There is no blood test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thomas-Maclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stoppard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Miedema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tatemichi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. Family Physician</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1102" to="1103" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Why has it taken so long for biological psychiatry to develop clinical tests and what to do about it?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Insel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1174" to="1179" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Physicians attitudes, diagnostic process and barriers regarding depression diagnosis in primary care: A systematic review of qualitative studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kantert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Family Practice</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="263" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clinical diagnosis of depression in primary care: A meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="issue">9690</biblScope>
			<biblScope unit="page" from="609" to="619" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The reliability and validity of discrete and continuous measures of psychopathology: A quantitative review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Markon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chmielewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The continuum of depressive states in the population and the differential diagnosis between &quot;Normal&quot;&apos; sadness and clinical depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sadness or Depression? International Perspectives on the Depression Epidemic and Its Meaning</title>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Performance of the PHQ-9 as a screening tool for depression after stroke</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stroke</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="635" to="638" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-report inventories in the study of depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pichot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Results in Depression Research</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="53" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rating scales for depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cusin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Clinical Rating Scales and Assessment in Psychiatry and Mental Health</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="7" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Assessing personality and psychopathology with self-report inventories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ben-Porath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Psychology. Hoboken</title>
		<meeting><address><addrLine>NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="553" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Screening and case-finding instruments for depression: A meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gilbody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>House</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. Med</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assoc</forename><forename type="middle">J</forename></persName>
		</author>
		<ptr target="http://www.cmaj.ca/content/178/8/997.abstract" />
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="997" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Performance of screening tools in detecting major depressive disorder among patients with coronary heart disease: A systematic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Browning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Sci. Monitor</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="646" to="653" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Symptom screening scales for detecting major depressive disorder in children and adolescents: A systematic review and meta-analysis of reliability, validity and diagnostic utility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stockings</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S016503271400785X" />
	</analytic>
	<monogr>
		<title level="j">J. Affective Disorders</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="447" to="463" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Screening for Depression in Clinical Practice: An Evidence-Based Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Coyne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Oxford Univ. Press</publisher>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Technological innovations in mental healthcare: Harnessing the digital revolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hollis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morriss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. J. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="265" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A primer on observational measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assessment</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="404" to="413" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automated audiovisual depression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="75" to="79" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<ptr target="http://ieeexplore.ieee.org" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<ptr target="http://link.springer.com" />
		<title level="m">Available</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiley</surname></persName>
		</author>
		<ptr target="http://onlinelibrary.wiley.com" />
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oxford</surname></persName>
		</author>
		<ptr target="http://www.oxfordjournals.org" />
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pubmed</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed" />
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scopus</surname></persName>
		</author>
		<ptr target="http://www.scopus.com" />
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Googlescholar</surname></persName>
		</author>
		<ptr target="http://scholar.google.gr" />
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Medpilot</surname></persName>
		</author>
		<ptr target="http://www.medpilot.de" />
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<ptr target="http://www.mayoclinic.org/diseases-conditions/depression/basics/tests-diagnosis/con-20032977" />
	</analytic>
	<monogr>
		<title level="j">Mayo Clinic</title>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Facial signs and psycho-physical status estimation for well-being assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chiarugi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Health Informat</title>
		<meeting>7th Int. Conf. Health Informat</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="555" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Extraction of facial features as indicators of stress and anxiety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pediaditis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th Annu. Int. Conf</title>
		<meeting>37th Annu. Int. Conf</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3711" to="3714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">AVEC 2013: The continuous audio/visual emotion and depression recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>3rd ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Workshop summary for the 3rd international audio/visual emotion challenge and workshop (AVEC&apos;13)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st ACM Int. Conf. Multimedia, 2013</title>
		<meeting>21st ACM Int. Conf. Multimedia, 2013</meeting>
		<imprint>
			<biblScope unit="page" from="1085" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">AVEC 2014: 3D dimensional affect and depression recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>4th ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Summary for AVEC 2016: Depression, mood, and emotion recognition workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">http://doi.acm.org/10.1145/2964284.2980532</idno>
		<ptr target="http://doi.acm.org/10.1145/2964284.2980532" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Conf</title>
		<meeting>ACM Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1483" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Effectiveness of web-delivered acceptance and commitment therapy in relation to mental health and well-being: A systematic review and meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glendenning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Hoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<ptr target="Art.no.e221" />
	</analytic>
	<monogr>
		<title level="j">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automatic behaviour understanding in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Roadmapping Future Multimodal Interaction Res</title>
		<meeting>Workshop Roadmapping Future Multimodal Interaction Res</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Technology tools supportive of DSM-5: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mental Health Practice in a Digital World: A Clinicians Guide</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="199" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Procedures for performing systematic reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kitchenham ; Keele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keele</forename><surname>Univ</surname></persName>
		</author>
		<ptr target="http://www.ifs.tuwien.ac.at/~weippl/systemicReviewsSoftwareEngineering.pdf" />
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Utilization of health-enabling technologies for early detection of symptom parameters of addiction and depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jâ‚¬ Ahne-Raden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scharnweber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haux</surname></persName>
		</author>
		<ptr target="http://person.hst.aau.dk/ska/mie2012/CD/Interface_MIE2012/MIE_2012_Content/MIE_2012_Content/sco.html" />
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Conf. Eur. Federation Med. Informat. Quality Life Through Quality Inf</title>
		<meeting>24th Int. Conf. Eur. Federation Med. Informat. Quality Life Through Quality Inf</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Automated mental state detection for mental health care</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>D'mello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence in Behavioral and Mental Health Care</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="117" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Acquisition of affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emotions and Personality in Personalized Services: Models, Evaluation and Applications</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="57" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Advances, challenges, and opportunities in automatic facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Face Detection and Facial Image Analysis</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="63" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The utility of facial analysis algorithms in detecting melancholia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Hyett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Face Detection and Facial Image Analysis</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="359" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A review of depression and suicide risk assessment using speech analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schnieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="10" to="49" />
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Survey on RGB, 3D, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Corneanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Simn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1568" />
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Non-Verbal Communication in Depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ellgring</surname></persName>
		</author>
		<editor>R. J. Eiser and K. R. Scherer</editor>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Therapist training in nonverbal communication. I: Nonverbal cues for depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Waxer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Classifying depression patients and normal subjects using machine learning techniques and nonlinear features from EEG signal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hosseinifard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rostami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="345" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Could fNIRS promote neuroscience approach in clinical psychology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adorni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brugnera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Compare</surname></persName>
		</author>
		<idno type="DOI">http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00456</idno>
		<ptr target="http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00456" />
	</analytic>
	<monogr>
		<title level="j">Frontiers Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">456</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multichannel near-infrared spectroscopy in depression and schizophrenia: Cognitive brain activation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mikuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="501" to="511" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Remission prognosis for cognitive therapy for recurrent depression using the pupil: Utility and neural correlates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Siegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Steinhauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Thase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="726" to="733" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pupillary reactivity to emotional information in child and adolescent depression: Links to clinical and ecological measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Silk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. J. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1873" to="1880" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Motivational and emotional influences on cognitive control in depression: A pupillometry study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Siegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mandell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Affective Behavioral Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="275" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Pupillometry in Chinese female patients with depression: A pilot study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Environ. Res. Public Health</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2236" to="2243" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Tackling mental health by integrating unobtrusive multimodal sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th AAAI Conf</title>
		<meeting>29th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1401" to="1408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Pupillary reactivity to negative stimuli prospectively predicts recurrence of major depressive disorder in women</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Kudinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Burkhouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Siegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Woody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Gibb</surname></persName>
		</author>
		<idno type="DOI">10.1111/psyp.12764</idno>
		<ptr target="http://dx.doi.org/10.1111/psyp.12764" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Alleviated negative rather than positive attentional bias in patients with depression in remission: An eye-tracking study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Int. Med. Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1072" to="1086" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">From anxious youth to depressed adolescents: Prospective prediction of 2-year depression symptoms via attentional bias measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="278" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Ocular motor differences between melancholic and non-melancholic depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winograd-Gurvich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Georgiou-Karistianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Millist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Affect. Disorders</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="203" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Self-paced and reprogrammed saccades: Differences between melancholic and non-melancholic depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winograd-Gurvich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Georgiou-Karistianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Millist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">B</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurosci. Res</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="260" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Facial Action Coding System: The Manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Res. Nexus Netw. Inf. Res</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Detecting depression from facial actions and vocal prosody</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Affect. Comput. Intell. Interaction Workshops</title>
		<meeting>3rd Int. Conf. Affect. Comput. Intell. Interaction Workshops</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">An approach for automatically measuring facial activity in depressed subjects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gâ‚¬ Ocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hyett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Affect. Comput. Intell. Interaction Workshops</title>
		<meeting>3rd Int. Conf. Affect. Comput. Intell. Interaction Workshops</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Automatic nonverbal behavior indicators of depression and PTSD: Exploring gender differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Humaine Assoc. Conf. Affect. Comput</title>
		<meeting>Humaine Assoc. Conf. Affect. Comput</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A multimodal context-based approach for distress assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. Multimodal Interaction</title>
		<meeting>16th Int. Conf. Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="240" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Automatic nonverbal behavior indicators of depression and PTSD: The effect of gender</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="29" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The computer analysis of facial expressions: On the example of depression and anxiety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mcintyre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, College Eng. Comput. Sci., Australian Nat. Univ</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Australia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Social signal processing in depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Workshop Social Signal Process</title>
		<meeting>2nd Int. Workshop Social Signal ess</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Facial response to video content in depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gâ‚¬ Ocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parker</surname></persName>
		</author>
		<ptr target="https://icmi.acm.org/2011/data/ICMI_2011_Technical_Program.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int. Conf. Multimodal Interaction Workshop: Inferring Cogn. Emotional States Multimodal Measures</title>
		<meeting>13th Int. Conf. Multimodal Interaction Workshop: Inferring Cogn. Emotional States Multimodal Measures</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Rosenwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="641" to="647" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Social risk and depression: Evidence from manual and automatic facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Rosenwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Conf. Autom. Face Gesture Recognit</title>
		<meeting>10th Int. Conf. Autom. Face Gesture Recognit</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Beyond group differences: Specificity of nonverbal behavior and interpersonal communication to depression severity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>3rd ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Vocal and facial biomarkers of depression based on motor incoordination and timing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Helfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ciccarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>4th ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Evaluation of the intricacies of emotional facial expression of psychiatric patients using computational models,&quot; in Understanding Facial Expressions in Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="199" to="226" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Coupled HMM-based multimodal fusion for mood disorder detection through elicited audio-visual signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12652-016-0395-y#citeas</idno>
		<ptr target="https://link.springer.com/article/10.1007/s12652-016-0395-y#citeas" />
	</analytic>
	<monogr>
		<title level="j">J. Ambient Intell. Humanized Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">AVEC 2016: Depression, mood, and emotion recognition workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>6th Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<idno type="DOI">http://doi.acm.org/10.1145/2988257.2988258</idno>
		<ptr target="http://doi.acm.org/10.1145/2988257.2988258" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Basic emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Cognition and Emotion</title>
		<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="45" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Automatic behavior descriptors for psychological disorder analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE Int. Conf. Workshops Autom. Face Gesture Recognit</title>
		<meeting>10th IEEE Int. Conf. Workshops Autom. Face Gesture Recognit</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Multimodal prediction of psychological disorders: Learning verbal and nonverbal commonalities in adjacency pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Workshop Semantics Pragmatics Dialogue</title>
		<meeting>17th Workshop Semantics Pragmatics Dialogue</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="160" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">SimSensei demonstration: A perceptive virtual human interviewer for healthcare applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Conf</title>
		<meeting>29th Conf</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4307" to="4308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Audiovisual behavior descriptors for depression assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Int. Conf. Multimodal Interaction</title>
		<meeting>15th Int. Conf. Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="135" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">The distress analysis interview corpus of human and computer interviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Language Resources Eval</title>
		<meeting>Language Resources Eval</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3123" to="3128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Automatic audiovisual behavior descriptors for psychological disorder analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="648" to="658" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Towards an affective interface for assessment of psychological distress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Affect. Comput. Intell. Interaction</title>
		<meeting>Int. Conf. Affect. Comput. Intell. Interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Eye movement analysis for depression detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alghowinem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gâ‚¬ Ocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4220" to="4224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Cross-cultural detection of depression from nonverbal behaviour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alghowinem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gâ‚¬ Ocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face Gesture Recognit</title>
		<meeting>Int. Conf. Autom. Face Gesture Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Multimodal prediction of affective dimensions and depression in human-computer interactions categories and subject descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>4th ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Head pose and movement analysis as an indicator of depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alghowinem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gâ‚¬ Ocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Humaine Assoc. Conf. Affect. Comput. Intell. Interaction</title>
		<meeting>Humaine Assoc. Conf. Affect. Comput. Intell. Interaction</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="283" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Depression analysis: A multimodal approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th ACM Int. Conf. Multimodal Interaction</title>
		<meeting>14th ACM Int. Conf. Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">An automated framework for depression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Humaine Assoc. Conf. Affect. Comput. Intell. Interaction</title>
		<meeting>Humaine Assoc. Conf. Affect. Comput. Intell. Interaction</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="630" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Can body expressions contribute to automatic depression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gâ‚¬ Ocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE Int. Conf. Workshops Autom. Face Gesture Recognit</title>
		<meeting>10th IEEE Int. Conf. Workshops Autom. Face Gesture Recognit</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Neural-net classification for spatio-temporal descriptor based depression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gâ‚¬ Ocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Pattern Recognit</title>
		<meeting>21st Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2634" to="2638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Relative body parts movement for automatic depression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Humaine Assoc. Conf. Affect. Comput. Intell. Interaction</title>
		<meeting>Humaine Assoc. Conf. Affect. Comput. Intell. Interaction</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="492" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Designing a framework for assisting depression severity assessment from facial image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pampouchidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tsiknakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabrice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Signal Image Process</title>
		<meeting>IEEE Int. Conf. Signal Image ess</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="578" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">American Psychiatric Association, Diagnostic and Statistical Manual of Mental Disorders</title>
		<imprint>
			<date type="published" when="1994" />
			<publisher>American Psychiatric Assoc</publisher>
			<pubPlace>St. Louis, MO, USA</pubPlace>
		</imprint>
	</monogr>
	<note>4th ed. Lake</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Multimodal depression detection: Fusion analysis of paralinguistic, head pose and eye gaze behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alghowinem</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2016.2634527</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Prediction of clinical depression in adolescents using facial image analaysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ooi Kuanee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><forename type="middle">A</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Allen</surname></persName>
		</author>
		<ptr target="http://toc.proceedings.com/21884webtoc.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Workshop Image Anal. Multimedia Interactive Serv</title>
		<meeting>12th Int. Workshop Image Anal. Multimedia Interactive Serv</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Handbook of Emotion Elicitation and Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Coan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Oxford Univ. Press</publisher>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Human Facial Expression: An Evolutionary View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Fridlund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Academic Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">The Behavioral Ecology View of Facial Displays, 25 Years Later</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Fridlund</surname></persName>
		</author>
		<ptr target="http://emotionresearcher.com/the-behavioral-ecology-view-of-facial-displays-25-years-later/" />
		<imprint>
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Depression recognition based on dynamic facial and vocal expression features using partial least square regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ai-Shuraifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>3rd ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Diagnosis of depression by behavioural signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sethu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>3rd ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Multimodal assistive technologies for depression diagnosis and monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="228" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Automatic depression scale prediction using facial expression dynamics and regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F A</forename><surname>Gaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Turabzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Workshop Audio/ Visual Emotion Challenge</title>
		<meeting>4th ACM Int. Workshop Audio/ Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Early prediction of clinical depression in adolescents using single-chanel and multichannel classification approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ooi Kuan Ee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, School Elect. Comput. Eng., RMIT Univ</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Emotion recognition and depression diagnosis by acoustic and visual features: A multimodal approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Minker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>4th ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Fusion of audio-visual features using hierarchical classifier systems for the recognition of affective states and the state of depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glodek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zharkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Pattern Recognit</title>
		<meeting>3rd Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="671" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Inferring depression and affect from application dependent meta knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>4th ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Model fusion for multimodal depression classification and level detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Senoussaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarria-Paja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>4th ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="57" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Detecting depression using multimodal approach of emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Meftah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Thanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Complex Syst</title>
		<meeting>Int. Conf. Complex Syst</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">EEG signal and video analysis based depression indication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Katryal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Alur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Menaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1353" to="1360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Using thermographic cameras to investigate eye temperature and clinical severity in depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Maller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Junor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Optics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Art. no. 026001</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Discriminating clinical phases of recovery from major depressive disorder using the dynamics of facial expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mayberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 38th</title>
		<meeting>38th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annu. Int. Conf. IEEE Eng. Med. Biol. Soc</title>
		<imprint>
			<biblScope unit="page" from="2254" to="2257" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Multimodal analysis of verbal and nonverbal behaviour on the example of clinical depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alghowinem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Canberra, ACT, Australia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Australian Nat. Univ., College of Engineering and Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Fusing affective dimensions and audio-visual features from segmented video for depression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Espinoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montes-Y G Omez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pinto-AvedaÃ±o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Reyes-Meza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>4th ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Multimodal detection of depression in clinical interviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dibeklio Glu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimodal Interaction</title>
		<meeting>ACM Int. Conf. Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="307" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Fusing active orientation models and mid-term audio features for automatic depression estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Smailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Perantonis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th ACM Int. Conf. PErvasive Technol. Related Assistive Environ</title>
		<meeting>9th ACM Int. Conf. PErvasive Technol. Related Assistive Environ</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Video-based detection of the clinical depression in adolescents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Maddage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Senaratne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><forename type="middle">A</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Int. Conf</title>
		<meeting>31st Int. Conf</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3723" to="3726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Depression estimation using audiovisual features and fisher vector encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>4th ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="87" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Multimodal sensing of affect intensity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th ACM Int. Conf. Multimodal Interaction</title>
		<meeting>18th ACM Int. Conf. Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="567" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<idno type="DOI">http://doi.acm.org/10.1145/2993148.2997622</idno>
		<ptr target="http://doi.acm.org/10.1145/2993148.2997622" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">A video-based facial behaviour analysis approach to melancholia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th IEEE Conf. Autom. Face Gesture Recognit</title>
		<meeting>12th IEEE Conf. Autom. Face Gesture Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Ensemble CCA for continuous emotion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>4th ACM Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A temporally piece-wise Fisher vector approach for depression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Affect. Comput. Intell. Interaction</title>
		<meeting>Int. Conf. Affect. Comput. Intell. Interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="255" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Video-based depression detection using local curvelet binary patterns in pairwise orthogonal planes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pampouchidou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 38th Annu. Int. Conf</title>
		<meeting>38th Annu. Int. Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3835" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Eyes whisper depression: A CCA based multimodal approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia</title>
		<meeting>Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="961" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Decision tree based depression classification from audio video and language information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Oveneke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahli</surname></persName>
		</author>
		<idno type="DOI">http://doi.acm.org/10.1145/2988257.2988269</idno>
		<ptr target="http://doi.acm.org/10.1145/2988257.2988269" />
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Workshop Audio/ Visual Emotion Challenge</title>
		<meeting>6th Int. Workshop Audio/ Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Automated depression diagnosis based on facial dynamic analysis and sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1432" to="1441" />
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Multimodal depression recognition with dynamic visual and audio cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Affect. Comput. Intell. Interaction</title>
		<meeting>Int. Conf. Affect. Comput. Intell. Interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="260" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Automated depression diagnosis based on deep networks to encode facial appearance and dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<idno>doi: 10.1109/ TAFFC.2017.2650899</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Depression assessment by fusing high and low level features from audio, video, and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pampouchidou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>6th Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<idno type="DOI">http://doi.acm.org/10.1145/2988257.2988266</idno>
		<ptr target="http://doi.acm.org/10.1145/2988257.2988266" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Multimodal and multiresolution depression detection from speech and facial landmark features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Nallan</forename><surname>Chakravarthula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>6th Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<idno type="DOI">http://doi.acm.org/10.1145/2988257.2988261</idno>
		<ptr target="http://doi.acm.org/10.1145/2988257.2988261" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Automated facial action coding system for dynamic analysis of facial expressions in neuropsychiatric disorders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Methods</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="256" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">OpenFace: An open source facial behavior analysis toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru Saitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Appl. Comput. Vis</title>
		<meeting>IEEE Winter Conf. Appl. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">The SEMAINE API: Towards a standards-based framework for building emotion-oriented systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schrâ‚¬</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances Human-Comput. Interaction</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">The computer expression recognition toolbox (CERT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops</title>
		<meeting>IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">A semi-automated system for accurate gaze coding in natural dyadic interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ACM Int. Conf. Multimodal Interaction</title>
		<meeting>15th ACM Int. Conf. Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="87" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Gaze estimation in the 3D space using RGB-D sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Funes-Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="216" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Dense 3D face alignment from 2D videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th IEEE Int. Conf. Workshops Autom. Face Gesture Recognit</title>
		<meeting>11th IEEE Int. Conf. Workshops Autom. Face Gesture Recognit</meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Face alignment through subspace constrained mean-shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1034" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Facial Action Coding System: Investigators Guide. Salt Lake City</title>
		<meeting><address><addrLine>UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Computer-aided detection and diagnosis for prostate cancer based on mono and multi-parametric MRI: A Rreview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>LemaÃ®tre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mart I</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freixenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Vilanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meriaudeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="8" to="31" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Automated video-based facial expression analysis of neuropsychiatric disorders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Methods</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="224" to="238" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Dynamic multimodal measurement of depression severity using deep autoencoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dibeklio Glu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2017.2676878</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Informat</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Recognizing facial expression: Machine learning and application to spontaneous behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lainscsek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="568" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">AAM derived face representations for robust facial action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Workshops Autom. Face Gesture Recognit</title>
		<meeting>IEEE Int. Conf. Workshops Autom. Face Gesture Recognit</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Detecting depression using vocal, facial and semantic communication cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<idno type="DOI">http://doi.acm.org/10.1145/2988257.2988263</idno>
		<ptr target="http://doi.acm.org/10.1145/2988257.2988263" />
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>6th Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Staircase regression in OA RVM, data selection and gender dependency in AVEC 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Workshop Audio/Visual Emotion Challenge</title>
		<meeting>6th Int. Workshop Audio/Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<idno type="DOI">http://doi.acm.org/10.1145/2988257.2988265</idno>
		<ptr target="http://doi.acm.org/10.1145/2988257.2988265" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">A demonstration of dialogue processing in Sim-Sensei Kiosk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Morbini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Devault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Annu. Meeting Special Interest Group Discourse Dialogue</title>
		<meeting>15th Annu. Meeting Special Interest Group Discourse Dialogue</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">SimSensei Kiosk: A virtual human interviewer for healthcare decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Devault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Auton. Agents Multi-Agent Syst</title>
		<meeting>Int. Conf. Auton. Agents Multi-Agent Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1061" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">MultiSense-Context-aware nonverbal behavior analysis framework: A psychological distress use case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="203" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Vocal biomarkers of depression based on motor incoordination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Helfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd ACM Int. Workshop Audio/ Visual Emotion Challenge</title>
		<meeting>3rd ACM Int. Workshop Audio/ Visual Emotion Challenge</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild challenge 2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ACM Int. Conf. Multimodal Interaction</title>
		<meeting>15th ACM Int. Conf. Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild challenge 2014: Baseline, data and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. Multimodal Interaction</title>
		<meeting>16th Int. Conf. Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="461" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Video and image based emotion recognition challenges in the wild: EmotiW 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimodal Interaction</title>
		<meeting>ACM Int. Conf. Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="423" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Toward machine emotional intelligence: Analysis of affective physiological state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vyzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1175" to="1191" />
			<date type="published" when="2001-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<monogr>
		<title level="m" type="main">Anastasia Pampouchidou received the bachelor&apos;s degree in applied informatics and multimedia, from the Technological Educational Institute of Crete, in 2004 and the master&apos;s degree in computer vision from the University of Burgundy, in 2011, where she is currently working toward the PhD degree since 2013. Her PhD studies are funded by the Greek State Scholarships Foundation, under the scholarship programme from the revenue of the legacy in memory of Mary Zaousi. Previously, she worked for the EU funded project SEMEOTICONS for a period of two years. Her research interests involve image processing, machine learning</title>
		<imprint/>
	</monogr>
	<note>facial expression analysis, and affective computing</note>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">His research has been supported by several federal and national grants and focuses on neuropsychological and brain imaging studies of reading and memory using Magnetoencephalography and MRI/fMRI with children and adults. Ongoing studies explore psychoeducational, emotional, and neurophysiological profiles associated with specific reading disability, ADHD and neurodegenerative disorders. He has also developed and adapted in Greek several psychometric instruments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Panagiotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simos received the PhD degree in experimental psychology-biopsychology from</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>Southern Illinois University ; University of Texas Houston Medical School, and Psychology, University of Crete ; University of Crete</orgName>
		</respStmt>
	</monogr>
	<note>He is currently professor of developmental neuropsychology in the School of Medicine. for cognitive and linguistic abilities across the life-span</note>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">), while during 2010-2015 actively participated in several other EC funded projects developing ICT technology focusing on medical image processing and personalized medicine</title>
		<ptr target="http://www.contracancrum.eu/andhttp://www.tumor-project.eu/" />
	</analytic>
	<monogr>
		<title level="m">Kostas Marias is an associate professor in image processing in the Informatics Engineering Department, Technological Educational Institute of Crete and since 2010 he is the head and founder of the Computational Biomedicine Laboratory, FORTH-ICS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>previously Biomedical Informatics Laboratory</orgName>
		</respStmt>
	</monogr>
	<note>He has published more than 150 papers in international journals, books, and conference proceedings focusing on medical image analysis, biomedical informatics and modelling for personalized medicine</note>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">His research interests were focused on image processing for non-conventional imaging systems (UV, IR, polarization) and more recently on medical/biomedical imaging. He coordinated an Erasmus Mundus Master in the field of Computer Vision and Robotics from 2006 to 2010 and he was the vice president for International Affairs for the University of Burgundy from 2010 to 2012. He has authored and co-authored more than 150 international publications and holds three patents</title>
	</analytic>
	<monogr>
		<title level="m">Fabrice Meriaudeau received both the master&apos;s degree in physics from Dijon University, France as well as an engineering degree (FIRST) in material sciences</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>He was a postdoc for a year with The Oak Ridge National Laboratory. He is currently &quot;professeur des Universites&quot; and was director of the Le2i (UMR CNRS), which has more than 200 staff members. he is on leave from &quot;Universit e de Bourgogne&quot; and has joined Universiti Teknologi PETRONAS as a professor</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
