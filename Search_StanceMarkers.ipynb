{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Search_StanceMarkers.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yubin-xing-usask/stance_markers/blob/main/Search_StanceMarkers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTeocAJw_NZm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffFglBRheGJH"
      },
      "source": [
        "# a function of removing namespace in XML files\r\n",
        "\r\n",
        "def remove_namespace(doc, namespace):\r\n",
        "    \"\"\"Remove namespace in the passed document in place.\"\"\"\r\n",
        "    ns = u'{%s}' % namespace\r\n",
        "    nsl = len(ns)\r\n",
        "    for elem in doc.getiterator():\r\n",
        "        if elem.tag.startswith(ns):\r\n",
        "            elem.tag = elem.tag[nsl:]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGOV92gJYQSm"
      },
      "source": [
        "# a function finding file locations\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "def location_xml(inpath):\r\n",
        "  \"\"\"\r\n",
        "  This function input a path that stores all xml RAs, \r\n",
        "  returns the xml file location\r\n",
        "  \"\"\"\r\n",
        "  path_list = []\r\n",
        "  file_names = os.listdir(inpath)\r\n",
        "  for name in file_names:\r\n",
        "    if name.endswith(\".xml\"):\r\n",
        "      file_location = str(inpath + \"/\" + name)\r\n",
        "      path_list.append(file_location)\r\n",
        "  return path_list\r\n",
        "\r\n",
        "# location_xml('drive/MyDrive/Mitacs_project/Citation/MasterList/RAs_Engineering')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRvhRgfReW29"
      },
      "source": [
        "# start extracting sentences with a ref tag\r\n",
        "import xml.etree.ElementTree as ET\r\n",
        "import re\r\n",
        "import spacy\r\n",
        "\r\n",
        "# save paragraphs with refs into a list\r\n",
        "p_list = []\r\n",
        "for f in location_xml('drive/MyDrive/Mitacs_project/Citation/MasterList/RAs_Engineering'):\r\n",
        "  tree = ET.parse(f, parser = ET.XMLParser(encoding = 'utf-8'))\r\n",
        "  root = tree.getroot()\r\n",
        "  remove_namespace(root, u'http://www.tei-c.org/ns/1.0')\r\n",
        "  \r\n",
        "  for child in root.findall(\".//ref[@type='bibr']..\"): \r\n",
        "    # this line might need to be modified by using this xpath: \".//p\" to locate all paragraphs\r\n",
        "    st=child.itertext()\r\n",
        "    childlist=list(st)\r\n",
        "    seperator=\", \"\r\n",
        "    seperator.join(childlist)\r\n",
        "    p_list.append(childlist) # the ref and pargraph strings are seperated with \",\"\r\n",
        "    # this step might need to be optimized\r\n",
        "\r\n",
        "# save all text of these paragraphs into a list, join paragraph and reference strings together\r\n",
        "plist_final=[]\r\n",
        "for li in p_list:\r\n",
        "    s=\"\"\r\n",
        "    lil=s.join(li)\r\n",
        "    plist_final.append(lil)\r\n",
        "# print(plist_final)\r\n",
        "# now we have a full list with all body texts which contains all references (i.e. citations)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_Ku2YVLl2M8"
      },
      "source": [
        "# this step is to use regex to locate all necessary citations\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2egIZqBql_oY"
      },
      "source": [
        "# with the locations of citations, find the stance markers, including:\r\n",
        "# Reporting verbs, boosters, hedges, attitude markers, others.\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtoBFckuWCpp"
      },
      "source": [
        "# !!! keep this cell\n",
        "# test 6 remove the ref tag from Grobid and use spacy tools to tokenize the sentences, \n",
        "# then use regular expression to find the reference tags\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "# remove the namespace \n",
        "# looks like this step is not necessary anymore if using regex to search for citations\n",
        "# but this function might be useful for future projects\n",
        "def remove_namespace(doc, namespace):\n",
        "    \"\"\"Remove namespace in the passed document in place.\"\"\"\n",
        "    ns = u'{%s}' % namespace\n",
        "    nsl = len(ns)\n",
        "    for elem in doc.getiterator():\n",
        "        if elem.tag.startswith(ns):\n",
        "            elem.tag = elem.tag[nsl:]\n",
        "\n",
        "# parse the XML file into an element Tree\n",
        "tree = ET.parse('drive/MyDrive/Mitacs_project/Citation/ELS_HS_AMP_2015_V2_2.tei.xml', parser = ET.XMLParser(encoding = 'utf-8'))\n",
        "root = tree.getroot()\n",
        "remove_namespace(root, u'http://www.tei-c.org/ns/1.0')\n",
        "\n",
        "# save paragraphs with refs into a list\n",
        "p_list = []\n",
        "for child in root.findall(\".//ref[@type='bibr']..\"): # this line might need to be modified by using this xpath: \".//p\" to locate all paragraphs\n",
        "    st=child.itertext()\n",
        "    childlist=list(st)\n",
        "    seperator=\", \"\n",
        "    seperator.join(childlist)\n",
        "    p_list.append(childlist)\n",
        "\n",
        "# save all text of these paragraphs into a list, join paragraph and reference strings together\n",
        "list1=[]\n",
        "for li in p_list:\n",
        "    s=\"\"\n",
        "    lil=s.join(li)\n",
        "    list1.append(lil)\n",
        "\n",
        "# tokenize every sentence with spacy\n",
        "list_sent=[]\n",
        "for each_p in list1:\n",
        "    nlp = spacy.load(\"en_core_web_sm\") \n",
        "    doc = nlp(each_p)\n",
        "    for sent in doc.sents:\n",
        "        list_sent.append(sent.text)\n",
        "\n",
        "# use regex find the references and save them into a single list for the next analysis\n",
        "\n",
        "refIN_sent=[]\n",
        "for each_sen in list_sent:\n",
        "    author = \"(?:[A-Z][A-Za-z'`-]+)\"\n",
        "    etal = \"(?:et al.?)\"\n",
        "    additional = \"(?:,? (?:(?:and |& )?\" + author + \"|\" + etal + \"))\"\n",
        "    year_num = \"(?:19|20)[0-9][0-9]\"\n",
        "    page_num = \"(?:, p.? [0-9]+)?\"  # Always optional\n",
        "    year = \"(?:, *\"+year_num+page_num+\"| *\\(\"+year_num+page_num+\"\\))\"\n",
        "    regex = \"(\" + author + additional+\"*\" + year + \")\"\n",
        "    matches = re.search(regex, each_sen)\n",
        "    if matches != None:\n",
        "        refIN_sent.append(each_sen)\n",
        "\n",
        "# annotate the sentences with a tag to identify it is intergral or non-integral citation\n",
        "\n",
        "ref_tags = refIN_sent\n",
        "have_tags = []\n",
        "for element in ref_tags:\n",
        "    year_paren=\"\"\"\\((?:19|20|)[0-9][0-9]\\)\"\"\"\n",
        "    yearmatches = re.search(year_paren, element)\n",
        "    if yearmatches != None:\n",
        "        element = \"<int>\" + element + \"</int>\"\n",
        "    else:\n",
        "        element = \"<non_int>\" + element + \"</non_int>\"\n",
        "    have_tags.append(element)\n",
        "print(have_tags)\n",
        "\n",
        "# use spacy to tokenize these sentences and save it into an array or CSV\n",
        "for se in refIN_sent:\n",
        "    doc_se = nlp(se)\n",
        "    #for token_se in doc_se:\n",
        "       # print(token_se.text,\",\", token_se.lemma_, \",\", token_se.pos_, \",\", token_se.tag_,  \",\", token_se.dep_,\n",
        "       #      \",\", token_se.shape_,  \",\", token_se.is_alpha,  \",\", token_se.is_stop)\n",
        "\n",
        "# don't forget to count the number of refs extracted by using regex, and compare it with Grobid results\n",
        "# see which one is better"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHWnvAZ3_zaa"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91GhMVd-WCpr"
      },
      "source": [
        "#regex patterns to search for citations\n",
        "author = \"(?:[A-Z][A-Za-z'`-]+)\"\n",
        "etal = \"(?:et al.?)\"\n",
        "additional = \"(?:,? (?:(?:and |& )?\" + author + \"|\" + etal + \"))\"\n",
        "year_num = \"(?:19|20)[0-9][0-9]\"\n",
        "page_num = \"(?:, p.? [0-9]+)?\"  # Always optional\n",
        "year = \"(?:, *\"+year_num+page_num+\"| *\\(\"+year_num+page_num+\"\\))\"\n",
        "regex = \"(\" + author + additional+\"*\" + year + \")\"\n",
        "\n",
        "matches = re.findall(regex, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_7Il0-_WCps"
      },
      "source": [
        "# remove a namespace from XML docs\n",
        "import xml.etree.ElementTree as ET\n",
        "def remove_namespace(doc, namespace):\n",
        "    \"\"\"Remove namespace in the passed document in place.\"\"\"\n",
        "    ns = u'{%s}' % namespace\n",
        "    nsl = len(ns)\n",
        "    for elem in doc.getiterator():\n",
        "        if elem.tag.startswith(ns):\n",
        "            elem.tag = elem.tag[nsl:]\n",
        "\n",
        "metadata = '/Users/user1/Desktop/Python/metadata.xml'\n",
        "tree = ET.parse(metadata)\n",
        "root = tree.getroot()\n",
        "\n",
        "remove_namespace(root, u'http://apple.com/itunes/importer')\n",
        "tree.write('/Users/user1/Desktop/Python/done.xml',\n",
        "       pretty_print=True, xml_declaration=True, encoding='UTF-8')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}